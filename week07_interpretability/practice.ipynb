{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep models interpretability\n",
    "This notebooks shows examples of approaches for interpretation of (pre-)trained model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import skimage\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess as sp\n",
    "    result = sp.run(\n",
    "        ['pip3', 'install', 'scikit-image'],\n",
    "        stdout=sp.PIPE, stderr=sp.PIPE\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(result.stdout.decode('utf-8'))\n",
    "        print(result.stderr.decode('utf-8'))\n",
    "    \n",
    "    import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_gpu():\n",
    "    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetCount\n",
    "    nvmlInit()\n",
    "\n",
    "    return np.argmax([\n",
    "        nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i)).free\n",
    "        for i in range(nvmlDeviceGetCount())\n",
    "    ])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_id = get_free_gpu()\n",
    "    device = 'cuda:%d' % (get_free_gpu(), )\n",
    "    print('Selected %s' % (device, ))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('WARNING: using cpu!')\n",
    "\n",
    "### please, don't remove the following line\n",
    "x = torch.tensor([1], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imagenet class names\n",
    "import requests\n",
    "response = requests.get('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json')\n",
    "class_labels = {int(key): value for key, (code, value) in response.json().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels[897]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task I: manual explanation with smoothgrad (2 points)\n",
    "\n",
    "Let us begin by implementing our own little explainer for [DenseNet121](https://arxiv.org/abs/1608.06993) pretrained on ImageNet.\n",
    "\n",
    "For the sake of simplicity, we're gonna rely on [SmoothGrad](https://arxiv.org/pdf/1706.03825.pdf) explainer - a simple average of gradients over noisy inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.densenet121(pretrained=True).train(False).to(device)\n",
    "model = torchvision.models.resnet50(pretrained=True).train(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://cdn2.adrianflux.co.uk/wp-fluxposure/uploads/2014/08/no-7.jpg -O img.jpg\n",
    "\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2)\n",
    "    probs = torch.softmax(model(image_tensor), dim=-1)[0]\n",
    "    \n",
    "for i, class_ix in enumerate(probs.argsort(descending=True)[:10]):\n",
    "    print(f\"#{i + 1}: p={probs[class_ix].item():.3f}\\t{class_labels[class_ix.item()]} ({class_ix.item()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement SmoothGrad itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_smoothgrad(image: torch.Tensor, class_ix, num_runs=32, sigma=0.1):\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2)  # shape: [1, 3, h, w]\n",
    "    \n",
    "    # 1. create :num_runs: copies of an image\n",
    "    # 2. apply independent random noise to each copy, use N(0, sigma) distribution\n",
    "    # 3. compute gradients of logit for class_ix w.r.t. input pixels\n",
    "    # 4. average these gradients across different images\n",
    "    # 5. re-shape these gradients in the same shape as original image: (h, w, 3)\n",
    "    \n",
    "    <YOUR CODE HERE>\n",
    "    \n",
    "    average_gradients = <...>\n",
    "    \n",
    "    assert tuple(average_gradients.shape) == image.shape\n",
    "    return abs(average_gradients).cpu().numpy()\n",
    "\n",
    "for class_ix in 163, 717, 897:  # <-- insert your classes here, use numbers in (brackets)\n",
    "    plt.title(f'Explaining \"{class_labels[class_ix]}\"')\n",
    "    plt.imshow(explain_smoothgrad(image, class_ix), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out!__ Peter Higgs nobel prize photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://i.pinimg.com/originals/3a/e1/83/3ae18369ab2e86be83e637ad702ec832.jpg -O img.jpg\n",
    "\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_tensor = torch.as_tensor(image, device=device, dtype=torch.float32)\n",
    "    image_tensor = image_tensor[None].permute(0, 3, 1, 2)\n",
    "    probs = torch.softmax(model(image_tensor), dim=-1)[0]\n",
    "    \n",
    "for i, class_ix in enumerate(probs.argsort(descending=True)[:10]):\n",
    "    print(f\"#{i + 1}: p={probs[class_ix].item():.3f}\\t{class_labels[class_ix.item()]} ({class_ix.item()})\")\n",
    "    \n",
    "for class_ix in 834, 457, 861:\n",
    "    plt.title(f'Explaining \"{class_labels[class_ix]}\"')\n",
    "    plt.imshow(explain_smoothgrad(image, class_ix), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHapley Additive exPlanations\n",
    "Now, let's try something heavier. The current state of the art in explaining model predictions is [SHAP](https://arxiv.org/abs/1705.07874): Shapley Additive Explanations.\n",
    "\n",
    "This method is based on [Shapley values](https://en.wikipedia.org/wiki/Shapley_value) - a game-theoretic concept that evaluates the contribution of individual players in a cooperative game. Except this time our \"players\" are input features and the \"game\" is predicting whichever output the model gave.\n",
    "\n",
    "Computing Shapley values naively requires $O(F!)$ time where F is the number of features. To make this computation more feasible, authors [proposed](https://arxiv.org/abs/1705.07874) several approximations, one of which relies on averaged gradients. This approximation also requires \"background\" data - other images similar to the ones in question that can be used as reference points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import shap\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess as sp\n",
    "    result = sp.run(\n",
    "        ['pip3', 'install', 'shap'],\n",
    "        stdout=sp.PIPE, stderr=sp.PIPE\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(result.stdout.decode('utf-8'))\n",
    "        print(result.stderr.decode('utf-8'))\n",
    "    \n",
    "    import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# load \"background\" images - some 50 random images from ImageNet\n",
    "background, _ = shap.datasets.imagenet50()\n",
    "background = torch.as_tensor(background / 255.0, device=device, dtype=torch.float32)\n",
    "background = background.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "background.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image to explain\n",
    "!wget -q https://i.pinimg.com/originals/32/da/5c/32da5c3314fcc5ebf1a7b7d1548fcb03.jpg -O img.jpg\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "image_tensor = torch.as_tensor(image[None], device=device, dtype=torch.float32).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# explain and visualize. If you're not using a powerful V100 GPU, this may take up to 20 minutes.\n",
    "explainer = shap.GradientExplainer((model, model.layer1), background)\n",
    "shap_values, indices = explainer.shap_values(image_tensor, ranked_outputs=5, nsamples=200)\n",
    "shap_values = [np.transpose(values, (0, 2, 3, 1)) for values in shap_values]\n",
    "index_names = np.vectorize(lambda i: class_labels[i])(indices.cpu().numpy())\n",
    "shap.image_plot(shap_values, image_tensor.permute(0, 2, 3, 1).cpu().numpy(), index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obligatory physicist reference\n",
    "!wget -q https://images-na.ssl-images-amazon.com/images/I/51ArQaCkkZL._AC_.jpg -O img.jpg\n",
    "image = resize(plt.imread('img.jpg')[..., :3] / 255.0, (224, 224))\n",
    "image_tensor = torch.as_tensor(image[None], device=device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "explainer = shap.GradientExplainer((model, model.layer1), background)\n",
    "shap_values, indices = explainer.shap_values(image_tensor, ranked_outputs=3, nsamples=200)\n",
    "shap_values = [np.transpose(values, (0, 2, 3, 1)) for values in shap_values]\n",
    "index_names = np.vectorize(lambda i: class_labels[i])(indices.cpu().numpy())\n",
    "shap.image_plot(shap_values, image_tensor.permute(0, 2, 3, 1).cpu().numpy(), index_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Explaining classical machine learning models\n",
    "\n",
    "Finally, let's see how SHAP explainers can be applied to more conventional machine learning models like gradient boosting. \n",
    "\n",
    "Spoiler: exactly the same from a user's perspective. However, this time we're gonna use a different Shapley approximation implemented in TreeExplainer. For a full set of available explainers, take a look at their official [examples page](https://github.com/slundberg/shap/tree/master/notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: the following cells requires JavaScript support and should work in Jupyter (not JupyterLab or cocalc)**. \n",
    "In that case try running it on Google colab: https://colab.research.google.com/github/yandexdataschool/mlhep2020-assignments/blob/master/notebooks/interpretability/interpretability_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost # pip install catboost\n",
    "\n",
    "X, y = shap.datasets.boston()\n",
    "ensemble = catboost.CatBoostRegressor(iterations=100, learning_rate=0.1)\n",
    "ensemble.fit(X, y, verbose=False, plot=False)\n",
    "explainer = shap.TreeExplainer(ensemble)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# visualize explanation of the first example\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain training data: each *column* is a rotated plot from above, stacked for all training samples\n",
    "# (this plot is interactive, hover mouse to see feature names)\n",
    "shap.force_plot(explainer.expected_value, shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task II: Explain Intermediate Layers of Resnet50 on ImageNet (5 points)\n",
    "\n",
    "Sometimes, computing explanations w.r.t. pixels is more difficult (and less interpretable) than using intermediate layers that are closer to the \"output\" layer. The trick is to compute low-resolution explanations of output w.r.t. intermediate layer and then resizing them to fit the original image.\n",
    "\n",
    "Each layer in deep CNN learns filters of increasing complexity. The first layers learn basic feature detection filters such as edges and corners. The middle layers learn filters that detect parts of objects â€” for faces, they might learn to respond to features like eyes and brows, and deeper layers capture entire objects.\n",
    "\n",
    "**Your task** is to explain ResNet50 model outputs with respect to the convolution module from 2nd  and 4th layer of the pretrained Resnet50 network using the same GradientExplainer. *The task may require some googling or browsing the [docs](https://shap.readthedocs.io/en/latest) :)*\n",
    "\n",
    "**Please use at least 10 images from across the internet (or from ImageNet) and compare the 2nd and 4th layer explanations on these images.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* SHAP explainer based on superpixels - [notebook](https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html)\n",
    "* SHAP reference notebook - [view on github](https://github.com/slundberg/shap/tree/master/notebooks)\n",
    "* More various explainers in [ELI5](https://github.com/TeamHG-Memex/eli5)\n",
    "* Same notebook on Google [Colab](https://colab.research.google.com/github/yandexdataschool/mlhep2020-assignments/blob/master/notebooks/interpretability/interpretability_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
