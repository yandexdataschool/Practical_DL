{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford question answering dataset (SQuAD)\n",
    "\n",
    "Today we are going to work with a popular NLP dataset.\n",
    "\n",
    "Here is the description of the original problem:\n",
    "\n",
    "```\n",
    "Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\n",
    "```\n",
    "\n",
    "\n",
    "We are not going to solve it :) Instead we will try to answer the question in a different way: given the question, we will find a **sentence** containing the answer, but not within the context, but in a **whole databank**\n",
    "\n",
    "Just watch the hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open('train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code here is very similar to `week5/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "for q in tqdm.tqdm_notebook(data['data']):\n",
    "    for p in q['paragraphs']:\n",
    "        token_counts.update(tokenize(p['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 4\n",
    "\n",
    "tokens = [w for w, c in token_counts.items() if c > min_count] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_size = len(tokens)+2\n",
    "\n",
    "token_to_id = {t: i + 2 for i,t in enumerate(tokens)}\n",
    "id_to_token = {i + 2: t for i,t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[id_to_token[42]]==42\n",
    "assert len(token_to_id)==len(tokens)\n",
    "assert 0 not in id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def build_dataset(train_data):\n",
    "    '''Takes SQuAD data\n",
    "    Returns a list of tuples - a set of pairs (q, a_+)\n",
    "    '''\n",
    "    data = []\n",
    "    for q in tqdm.tqdm_notebook(train_data):\n",
    "        for p in q['paragraphs']:\n",
    "            offsets = []\n",
    "            curent_index = 0\n",
    "            for sent in sent_tokenize(p['context']):\n",
    "                curent_index+=len(sent)+2\n",
    "                offsets.append((curent_index, sent))\n",
    "                \n",
    "            for qa in p['qas']:\n",
    "                answer = qa['answers'][0]\n",
    "                found = False\n",
    "                for o, sent in offsets:\n",
    "                    if answer['answer_start']<o:\n",
    "                        data.append((qa['question'], sent))\n",
    "                        found = True\n",
    "                        break\n",
    "                assert found\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(data['data'], test_size=0.1)\n",
    "\n",
    "data_train = build_dataset(train_data)\n",
    "data_val = build_dataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_val[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, UNK=1, PAD=0):\n",
    "    '''This function gets a string array and transforms it to padded token matrix\n",
    "    Remember to:\n",
    "     - Transform a string to list of tokens\n",
    "     - Transform each token to it ids (if not in the dict, replace with UNK)\n",
    "     - Pad each line to max_len'''\n",
    "    token_matrix = []\n",
    "    \n",
    "    for s in strings:\n",
    "        seq = [token_to_id.get(token,UNK) for token in tokenize(s)]\n",
    "        token_matrix.append(seq)\n",
    "    \n",
    "    max_len = max(map(len,token_matrix))\n",
    "        \n",
    "    # handle empty batch\n",
    "    if max_len == 0:\n",
    "        max_len = 1\n",
    "    \n",
    "    for i in range(len(token_matrix)):\n",
    "        while(len(token_matrix[i]) < max_len):\n",
    "            token_matrix[i].append(PAD)\n",
    "    \n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_to_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-576bd96fdc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Hello, adshkjasdhkas, world\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correct!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token_to_id' is not defined"
     ]
    }
   ],
   "source": [
    "test = vectorize([\"Hello, adshkjasdhkas, world\", \"data\"], token_to_id, 1)\n",
    "assert test.shape==(2,3)\n",
    "assert (test[:,1]==(1,0)).all()\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "The beginning is same as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "margin = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_encoder(lstm_size=50, embeddings_size=50, target_space_dim=50, PAD=0):\n",
    "    '''\n",
    "    Build a lasagne network that converts input sequence to a fixed-size vector.\n",
    "    Must have a single input layer that accepts int32[batch,max_len]\n",
    "    '''\n",
    "    inp = InputLayer([None, None], dtype='int32')\n",
    "    mask = ExpressionLayer(inp, lambda ix: T.neq(ix,PAD))\n",
    "\n",
    "    net = EmbeddingLayer(inp, dict_size, embeddings_size)\n",
    "    net = LSTMLayer(net, lstm_size, mask_input=mask, only_return_final=True)\n",
    "    net = DenseLayer(net, target_space_dim)        \n",
    "\n",
    "    return net\n",
    "\n",
    "question_encoder = build_encoder()\n",
    "answer_encoder = build_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a single encoder for both poitive and negative answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = T.imatrix(name=\"word_ids_questions\")\n",
    "answers_positive = T.imatrix(name=\"word_ids_answers_positive\")\n",
    "answers_negative = T.imatrix(name=\"word_ids_answers_negative\")\n",
    "\n",
    "positive_output = get_output(answer_encoder,answers_positive)\n",
    "negative_output = #YOUR CODE: answer decoder's vector for negative answers\n",
    "anchor_output = #YOUR CODE: question decoder's vector for questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute dot products to get similarity. Also: you can use T.batched_dot for speed\n",
    "positive_dot = T.sum(anchor_output*positive_output, axis=1)\n",
    "negative_dot = T.sum(anchor_output*negative_output, axis=1)\n",
    "\n",
    "\n",
    "# compute triplet loss (pairwise hinge loss) as per formulae in the lecture.\n",
    "# please use T.maximum and not T.max!\n",
    "loss = #YOUR CODE\n",
    "\n",
    "recall = T.mean(positive_dot > negative_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allparams = get_all_params([answer_encoder,question_encoder],trainable=True)\n",
    "\n",
    "updates = lasagne.updates.adam(loss, allparams)\n",
    "train_op = theano.function([questions, answers_positive, answers_negative],\n",
    "                           [loss, recall],\n",
    "                           updates=updates)\n",
    "\n",
    "validate_op = theano.function([questions, answers_positive, answers_negative], [loss, recall])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "def iterate_batches(data, only_positives=False):\n",
    "    \"\"\"Takes a D\n",
    "    Returns a dict, containing pairs for each input type\n",
    "    only_positives indicates either we need to iterate over triplets vs only positive (needed for index)\n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        data_batch = data[i:i+batch_size]\n",
    "        \n",
    "        batch['positive'] = vectorize([sample[1] for sample in data_batch], token_to_id)\n",
    "        if not only_positives:\n",
    "            batch['anchor'] = vectorize([sample[0] for sample in data_batch], token_to_id)\n",
    "            batch['negative'] = vectorize([ data[np.random.randint(0, len(data))][1]  for i in range(len(data_batch))], \\\n",
    "                                          token_to_id, 1)\n",
    "        \n",
    "        yield batch\n",
    "        i+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    total_loss, total_recall = 0, 0\n",
    "    batches = 0\n",
    "    for batch in  iterate_batches(data_val):\n",
    "        batches+=1\n",
    "        current_loss, current_recall =  validate_op(batch['anchor'],\n",
    "                                                    batch['positive'],\n",
    "                                                    batch['negative'])\n",
    "        total_loss+=current_loss\n",
    "        total_recall+=current_recall\n",
    "        \n",
    "    total_loss/=batches\n",
    "    total_recall/=batches\n",
    "    \n",
    "    if total_recall > 0.9:\n",
    "        print('Cool! If recall is right, you earned (3 pts)')\n",
    "    return (total_loss, total_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "step = 0\n",
    "for j in range(num_epochs):\n",
    "    for i, (batch) in  enumerate(iterate_batches(data_train)):\n",
    "        current_loss, current_recall =  train_op(batch['anchor'],\n",
    "                                                 batch['positive'],\n",
    "                                                 batch['negative'])\n",
    "        step+=1\n",
    "        print(\"Current step: %s. Current loss is %s, Current recall is %s\" % (step, current_loss, current_recall))\n",
    "        if i%100==0:\n",
    "            print(\"Validation. Loss: %s, Recall: %s\" %validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Index(object):\n",
    "    \"\"\"Represents index of calculated embeddings\"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Class constructor takes a dataset and stores all unique sentences and their embeddings\"\"\"\n",
    "        self.vectors = []\n",
    "        self.sent = []\n",
    "        batch = []\n",
    "        i = 0\n",
    "        while True:\n",
    "            if data[i][1] not in self.sent:\n",
    "                self.sent.append(data[i][1])\n",
    "                batch.append(data[i][1])\n",
    "            if len(batch)>=batch_size or (len(batch) >0 and i+1==len(data)):\n",
    "                vectorized_batch = vectorize(batch, token_to_id=token_to_id, UNK=1)\n",
    "                self.vectors.extend(sess.run(positive_output, {inputs['positive'][0]: vectorized_batch[0],\n",
    "                                                               inputs['positive'][1]: vectorized_batch[1]\n",
    "                                                            }))\n",
    "                batch = []\n",
    "            if i+1==len(D):\n",
    "                break\n",
    "            i+=1\n",
    "        self.vectors = np.asarray(self.vectors)\n",
    "        \n",
    "    def predict(self, query, top_size =1):\n",
    "        \"\"\"\n",
    "        Function takes:\n",
    "         - query is a string, containing question\n",
    "        Function returns:\n",
    "         - a list with len of top_size, containing the closet answer from the index\n",
    "        You may want to use np.argpartition\n",
    "          \"\"\"\n",
    "        vectorized_batch = vectorize([query], token_to_id=token_to_id, UNK=1)\n",
    "        embedding =  sess.run(anchor_output, {inputs['anchor'][0]: vectorized_batch[0],\n",
    "                                                    inputs['anchor'][1]: vectorized_batch[1]})[0]\n",
    "        scores = 1-self.vectors.dot(embedding)\n",
    "        indices = np.argpartition(scores, top_size)[:top_size]\n",
    "        indices = sorted(indices, key=lambda ind: scores[ind])\n",
    "        return [self.sent[i] for i in indices]\n",
    "    \n",
    "    def calculate_FHS(self, D):\n",
    "        \"\"\"Prototype for home assignment. Returns a float number\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = Index(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(index.vectors) == len(index.sent)\n",
    "assert type(index.sent[1])==str\n",
    "assert index.vectors.shape == (len(index.sent), target_space_dim)\n",
    "p  = index.predict(\"Hey\", top_size=3)\n",
    "assert len(p) == 3\n",
    "assert type(p[0])==str\n",
    "assert index.predict(\"Hello\", top_size=50)!=index.predict(\"Not Hello\", top_size=50)\n",
    "print(\"Ok (2 pts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index.predict('To show their strength in the international Communist movement, what did China do?', top_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_val[np.random.randint(0, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Home assignment\n",
    "**Task 1.** (3 pts) Implement **semihard** sampling strategy. Use **in-graph** sampling. You have a prototype above\n",
    "\n",
    "**Task 2.1.** (1 pt) Calculate a **FHS** (First Hit Success) metric on a whole validation dataset (over each query on whole `data_val` index). Prototype of the function in in `Index` class. Compare different model based on this metric. Add table with FHS values to your report.\n",
    "\n",
    "**Task 2.2.** Add calculation of other representative metrics. You may want to calculate different recalls on a mini-batch, or some ranking metrics.   \n",
    "\n",
    "**Task 3.** (2 pt) Do experiments with deep architecture and find out the best one. Analyse your results and write a conclusion. \n",
    "\n",
    "**describe your results here**\n",
    "\n",
    "Bonus task 1. (2++ pts) Add manual negatives to the model. What can be a good manual negative in this case?\n",
    "\n",
    "Bonus task 2. (2++ pts) Implement more efficient Nearest Neighbors Search method. How well it performs on our dataset?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {
    "69ee5b52104d471ca7bfb32ba4309743": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "7b18f460e231498eaafa7653026e98e0": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
