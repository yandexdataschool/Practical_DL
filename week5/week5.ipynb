{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#theano imports\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n",
    "floatX = theano.config.floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Про библиотеку\n",
    "\n",
    "Для этого семинара вам понадобится библиотека AgentNet\n",
    "\n",
    "В предположении, что вы уже имеете Bleeding Edge версию theano и lasagne, доставить её на Linux и Mac OS можно вот как:\n",
    "\n",
    "```\n",
    "pip install --upgrade https://github.com/yandexdataschool/AgentNet/archive/3e32be8c348f93a84541cc1c52f398cbdf05ec3c.zip\n",
    "```\n",
    "** Внимание: текущий семинар не совместим с python2 **\n",
    "\n",
    "В предположении, что python - это тот питон, которым вы пользуетесь в тетрадке.\n",
    "Если у вас в тетрадке используется другой питон - используйте его, например\n",
    "* если вы используете Py3, а по умолчанию стоит Py2 - `python3 setup.py install`\n",
    "* `sudo jupyter notebook` -> `sudo python setup.py install`\n",
    "* если вдруг возникают проблемы, которые не решаются комбинацией sudo и python2/python3 - пишите, мы вас вылечим :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Небольшой варнунг\n",
    "\n",
    "Задача, которая решается в этом семинаре - довольно простая, а сети - ничтожно маленькие. \n",
    "\n",
    "Вы, конечно, можете использовать GPU, но не факт, что она даст вам выигрыш в рантайме, а вот при компиляции от неё можно точно ожидать замедления процесса раза в 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack-augmented RNN\n",
    "![caption](https://usercontent1.hubstatic.com/6172838_f260.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегодня в меню\n",
    "\n",
    "__Задача__ - научить нейронку порождать последовательности вида `|`$ a^n b^m c^{n+m} $\n",
    " * n и m - целые числа в определённом интервале\n",
    " * Чего хотим:\n",
    "  * Чтобы последовательность имела правильную форму - `|`, сколько-то a, сколько-то b, сколько-то c\n",
    "     * ||aaacbbcba - не ок\n",
    "  * Чтобы количество букв C было как можно ближе к сумме количеств букв a и b\n",
    "     * в идеале, чтобы оно совпадало\n",
    "     \n",
    "     \n",
    "Что попробуем:\n",
    " * Ванильный RNN\n",
    " * Stack-augmented RNN\n",
    " \n",
    "Идейно мы будем учить их в режиме Language Model (см. предыдущий семинар) - читать последовательность и предсказывать следующий символ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Но для начала сделаем генератор \"правильных\" последовательностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_sequence(batch_size = 10,crop_length = 100 ):\n",
    "    \"\"\"\n",
    "    Generates sequence from pattern [0  1*n 2*m 3*(n+m)]\n",
    "    \"\"\"\n",
    "    sequences=[]\n",
    "    for i in range(batch_size):\n",
    "        seq = [0]\n",
    "        \n",
    "        \n",
    "        #заполните строку seq последовательностями длины ровно crop_length\n",
    "        #из подстрок вида [0  1*n 2*m 3*(n+m)],\n",
    "        # где n,m - случайные числа от 1 до 15 включительно\n",
    "        #примерs правильных строк есть на 2 клетки ниже после \"ожидаемый результат\"\n",
    "        \n",
    "        <ваш код тут>\n",
    "\n",
    "        \n",
    "        \n",
    "        assert len(seq) == crop_length\n",
    "        \n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences,dtype='int32')\n",
    "\n",
    "alphabet = np.array(list('|abc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "map(''.join,map(alphabet.__getitem__,generate_sequence(25,100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ожидаемый результат__ табы сверху - список из строчек вида\n",
    "\n",
    "\n",
    "`\n",
    " ...\n",
    " '||aaaaaaaaabbbcccccccccccc|aaaabccccc|aaaaaaaaabbccccccccccc|aaaaaaaaabbbbccccccccccccc|aaaaaaaaaabb',\n",
    " '||aaaaaaaaaaaabbbbbbbbbbcccccccccccccccccccccc|aaaabbbbbbbbbbbbcccccccccccccccc|aaaabbbbbbbbcccccccc',\n",
    " '||aaaaaaabbbbbcccccccccccc|aaaaabbbbbbccccccccccc|aaaaaaaaaaaabbbbbccccccccccccccccc|abbbbbbbbbbbccc',\n",
    " '||aaaaaaaaaaaabbbbcccccccccccccccc|aaaaaaaabbbbbbbbcccccccccccccccc|aaaaaaaaaaaabbbbbbbbcccccccccccc',\n",
    " '||aaaaaaaaaaaaabcccccccccccccc|aaaaaaaaaaabbbbbbbbbbbbccccccccccccccccccccccc|aaaaaaaaaaaaaabbbbbbbb',\n",
    " '||abbbbccccc|aaaaabbbbbbbbbbbbccccccccccccccccc|aaaaaaaaaaaaabbbbbbbbbbbbccccccccccccccccccccccccc|a',\n",
    " '||aaaaaaaabbbbcccccccccccc|aaaabbbbbbbbbccccccccccccc|aaaaaaaaaaaabbbbbbbbbbbccccccccccccccccccccccc',\n",
    " '||aaaaaaaabbbbbbbbbbbbbccccccccccccccccccccc|aaaaaabbbbbbbbbbbbbbcccccccccccccccccccc|aaaaaaaaaaaaab',\n",
    " ...\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from metrics import get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#формальные тестики\n",
    "gen_sequences = generate_sequence(1000,500)\n",
    "correctness_ratio, c_count_mae = get_metrics(gen_sequences,alphabet)\n",
    "\n",
    "# Все порожлённые последовательности должны иметь правильный формат - | a+ b+ c+\n",
    "assert correctness_ratio == 1.0\n",
    "# Во всех последовательносях количество букв C должно равняться сумме количеств букв a и b\n",
    "assert c_count_mae == 0\n",
    "\n",
    "#и выборка должна иметь правильный размер\n",
    "assert len(gen_sequences) == 1000\n",
    "assert len(gen_sequences[0]) == 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Константы и глобальные переменные\n",
    "\n",
    "* Просто несколько чиселок, которые будут использоваться далее по коду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Длина генерируемых последовательностей\n",
    "SEQ_LENGTH = 100\n",
    "\n",
    "# сколько последовательностей в одном минибатче\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# сколько итераций всего нужно провести\n",
    "N_EPOCHS = 5000\n",
    "\n",
    "#раз в какое количество эпох нужно печатать качество нейронки\n",
    "REPORT_RATE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Последовательность букв на вход\n",
    "sequences_batch = T.matrix(dtype=\"int32\",name=\"reference_sequences\")\n",
    "\n",
    "#Её размер (theano-выражение)\n",
    "batch_size = sequences_batch.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поучим RNN\n",
    "\n",
    "Структурно нам нужно создать вот такую махину, которая будет описывать 1 шаг RNN во времени\n",
    "\n",
    "* время - слева направо\n",
    "* входы - снизу, выходы - сверху\n",
    "\n",
    "![scheme](./rnn.png)\n",
    "\n",
    "где \n",
    "* prev rnn state - предыдущее состояние RNN\n",
    "* input letter - буква на входе\n",
    "* next rnn state - следующее состояние RNN\n",
    "* generate_letter - выбор одной конкретной следующей буквы пропорционально предсказанным вероятностям\n",
    "* всё остальное - слои лазаньи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import DenseLayer, ElemwiseSumLayer, InputLayer, EmbeddingLayer, NonlinearityLayer\n",
    "import agentnet\n",
    "from agentnet.resolver import ProbablisticResolver\n",
    "from agentnet.agent import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#observation = input letter - сюда на каждом шаге подаётся очередная буква из входной последовательности\n",
    "output_shape = (None,)\n",
    "\n",
    "observation_layer = InputLayer(output_shape,name=\"obs_input\")\n",
    "\n",
    "\n",
    "# embedding\n",
    "n_tokens = len(alphabet)\n",
    "obs_embedding = EmbeddingLayer(observation_layer,\n",
    "                               input_size=n_tokens,\n",
    "                               output_size=n_tokens,\n",
    "                               name = \"input_embedding\")\n",
    "\n",
    "\n",
    "\n",
    "# RNN memory\n",
    "\n",
    "#число нейронов в скрытом слое\n",
    "n_hid_1 = 70\n",
    "\n",
    "#Сюда прицепится предыдущее состояние RNN\n",
    "#~ prev rnn state\n",
    "prev_rnn_layer = InputLayer((None,n_hid_1),name=\"prev_rnn_state\")\n",
    "\n",
    "\n",
    "#Построим RNN вручную - по схеме выше\n",
    "\n",
    "rnn_frominput = полносвязный слой без нелинейности размером n_hid_1, растёт из embedding-а\n",
    "\n",
    "rnn_fromhidden = полносвязный слой без нелинейности размером n_hid_1, растёт из prev rnn layer-а\n",
    "\n",
    "#если не хотите учить RNN с двумя наборами b-шек, скажите одному из входов b=None\n",
    "\n",
    "\n",
    "rnn_sum = поэлементная сумма их (ElemwiseSumLayer)\n",
    "\n",
    "rnn = нелинейность (по умолчанию лучше взять tanh)\n",
    "\n",
    "\n",
    "\n",
    "# В этом словаре лежат пары  {выход скрытого состояния: вход скрытого состояния с предыдущего шага}\n",
    "\n",
    "from collections import OrderedDict\n",
    "memory_dict = OrderedDict([\n",
    "            (rnn,prev_rnn_layer),\n",
    "    ])\n",
    "\n",
    "\n",
    "#Вероятности букв\n",
    "\n",
    "probability_layer = DenseLayer(rnn,\n",
    "                               num_units = n_tokens,\n",
    "                               nonlinearity=  lasagne.nonlinearities.softmax,\n",
    "                               name=\"policy_original\")\n",
    "\n",
    "#resolver - выбирает конкретную букву пропорционально вероятностям\n",
    "\n",
    "resolver = ProbablisticResolver(probability_layer,\n",
    "                                assume_normalized=True,\n",
    "                                name=\"resolver\")\n",
    "\n",
    "\n",
    "#Проверка, совпадают ли формы у состояний. Должна пройти, если вы не сделали чего-то ужасного\n",
    "assert tuple(lasagne.layers.get_output_shape(resolver)) == tuple(output_shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Создаём рекуррентный генератор, который\n",
    "# - имеет 1 вход - observation layer\n",
    "# - имеет память, описанную в memory dict ( один слой RNN )\n",
    "# - генерирует буквы с вероятностями, полученными на probability layer\n",
    "# - принимает решение, какую букву генерить, на слое resolver layer - пропорционально вероятностям\n",
    "\n",
    "agent = Generator(\n",
    "    observation_layer,\n",
    "    memory_dict,\n",
    "    probability_layer,\n",
    "    resolver\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# А теперь покрутим его\n",
    "\n",
    "* Применим агента к последовательности букв на входе по схеме выше\n",
    "* Получим тот же самый формат, что и у lasagne.layers.RecurrentLayer, GRU и LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessions = agent.get_sessions(session_length=SEQ_LENGTH,\n",
    "                             recorded_sequences=sequences_batch,\n",
    "                             batch_size=batch_size,)\n",
    "\n",
    "\n",
    "# состояние RNN - сгенерирнованные буквы  - вероятности букв\n",
    "agent_states,          action_seq,           probas_seq       =  sessions\n",
    "\n",
    "\n",
    "# Вытаскиваем состояние конкретного RNN\n",
    "rnn_seq = agent_states[rnn]\n",
    "\n",
    "\n",
    "# Из всего этого нам нужны только вероятности :)\n",
    "# probas_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А дальше всё как в обычной лазанье"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Получим веса\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_weights = int(T.sum([T.prod(w.shape) for w in weights]).eval())\n",
    "print \"Всего весов:\", total_weights\n",
    "\n",
    "#если вы экспериментируете с размером нейронки - удалите следующую строку\n",
    "assert  5200 < total_weights <= 5700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь\n",
    "\n",
    "На этот раз для ускорения обучения мы предсказываем не 1 последний символ, а предсказываем следующий символ в каждый момент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Возьмём все предсказания кроме последнего(для него мы не знаем \"следующего\")\n",
    "predicted_probas = probas_seq[:,:-1]\n",
    "\n",
    "# введём минимальную вероятность - чтобы не получить -Inf в логарифме\n",
    "predicted_probas = T.maximum(predicted_probas,1e-10)\n",
    "\n",
    "# Правильные ответы - для 0-го выхода - первый вход, для 1-го - второй вход и так далее\n",
    "# нулевой вход нам не нужен\n",
    "references = sequences_batch[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "# Обычная кроссэнтропия\n",
    "model_loss = lasagne.objectives.categorical_crossentropy(\n",
    "    predicted_probas.reshape([-1,n_tokens]),\n",
    "    references.ravel()\n",
    ").mean()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Регуляризатор для вкуса\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-5\n",
    "loss = model_loss + reg_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = ваш любивый оптимизатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скомпилируем это всё.\n",
    "* В первый раз, если SEQ_LENGTH больше 25, можно успеть выпить кофе;\n",
    "* Если вы на ещё и на GPU - съесть тортик.\n",
    "* К слову, у него тоже есть слои\n",
    "\n",
    "![canvas](http://www.rabstol.net/uploads/gallery/main/322/rabstol_net_cakes_30.jpg)\n",
    "\n",
    "* p.s Cake is a lie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([sequences_batch],[loss],updates=updates)\n",
    "evaluation_fun = theano.function([sequences_batch],[loss,model_loss,reg_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация новых символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# сюда мы подадим размер батча для генерации\n",
    "gen_batch_size = T.scalar('generated batch size','int32')\n",
    "\n",
    "\n",
    "# аналогично предыдущему, но теперь мы не даём ему \"правильную\" последовательность, \n",
    "# и он кушает свои собственные выходы с предыдущего момента времени\n",
    "_,generated_action_seq,_ = agent.get_sessions(session_length=SEQ_LENGTH,\n",
    "                             batch_size=gen_batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#допиваем кофе\n",
    "get_sequences = theano.function([gen_batch_size],generated_action_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Пока наша сетка ничено не знает (как Jon Snow, только не <spoiler>)\n",
    "map(alphabet.__getitem__,get_sequences(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Цикл обучения\n",
    "\n",
    "* Всё как обычно - учимся и раз в 100 итераций считаем метрики\n",
    "\n",
    "* Метрики нас интересуют 3 - \n",
    " * llh - простой loglikelihood - должен падать +- шум\n",
    " * доля правильных последовательностей - с какой вероятностью rnn сгенерит подряд сколько-то a, потом b, потом c - и потом начнёт новую строку.\n",
    "   * для формалистов - доля строк, которые полностью матчатся регээкспом \"^|a+b+c+\"\n",
    "   * тоже должен в целом расти, пока не выйдёт на асимптоту близь единицы\n",
    " * ошибка в количестве C\n",
    "   * рассматриваем только \"правильные\" последовательности по определению из предыдущего пункта\n",
    "   * считаем количество букв a (n), количество букв b (m). Правильное количество букв с - n+m\n",
    "   * Считаем средний модуль разности между сгенерированным количеством С и правильным.\n",
    "   * Формально метрика называется mean absolute error или MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "metrics = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(N_EPOCHS):\n",
    "    \n",
    "    #породим строки\n",
    "    new_batch = generate_sequence(BATCH_SIZE,SEQ_LENGTH)\n",
    "    #Потренируем чудовище\n",
    "    train_fun(new_batch)\n",
    "    \n",
    "    #Once in a while, напечатаем метрики\n",
    "    if i % REPORT_RATE==0:\n",
    "        \n",
    "        loss_components = evaluation_fun(new_batch)\n",
    "        print \"iter:%i\\tfull:%.5f\\tllh:%.5f\\treg:%.5f\"%tuple([i]+map(float,loss_components))        \n",
    "        \n",
    "        metrics['crossentropy'][i]=float(loss_components[1])\n",
    "        \n",
    "\n",
    "        examples = get_sequences(1000)\n",
    "        \n",
    "        correctness_ratio,c_count_mae = get_metrics(examples,alphabet)\n",
    "        \n",
    "\n",
    "        metrics[\"correctness_rates\"][i] = correctness_ratio\n",
    "        metrics[\"c_count_errors\"][i]=c_count_mae\n",
    "        \n",
    "        print \"Доля последовательностей правильной формы: %.5f\"%(correctness_ratio)\n",
    "        print \"MAE Ошибка по количеству C среди правильных: %.5f\"%(c_count_mae)\n",
    "        \n",
    "        for tid_line in examples[:3]:\n",
    "            print ' '.join(map(alphabet.__getitem__,tid_line))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    plt.figure(figsize=[14,8])\n",
    "    plt.plot(*zip(*sorted(metrics[metric].items(),key=lambda (k,v):k)),label=metric)\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel(\"popugai\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тестики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_5_last = lambda metric_name: map(lambda v: v[1],sorted(metrics[metric_name].items(),key=lambda v:v[0])[-5:])\n",
    "\n",
    "\n",
    "#проверяем, что минимум по последним 5 значениям метрики не хуже порога\n",
    "\n",
    "assert min(get_5_last(\"crossentropy\")) <= 0.3\n",
    "assert min(get_5_last(\"c_count_errors\")) <= 3\n",
    "assert min(get_5_last(\"correctness_rates\")) >= 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#запомним их в отдельную переменную\n",
    "rnn_metrics = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack RNN\n",
    "\n",
    "Теперь заведём Stack-augmented RNN.\n",
    "\n",
    "Оригинальная статья - http://arxiv.org/abs/1503.01007\n",
    "(там есть картинки)\n",
    "\n",
    "Идея - давайте мы выделим нашей RNN-ке стэк, которым она сможет управлять при помощи трёх операций\n",
    "\n",
    "* push - сдвиг стэка на единицу вглубь, добавление входного элемента\n",
    "* pop - сдвиг всех элементов на единицу ближе к выходу\n",
    "* no-op - сохранение состояния стэка\n",
    "\n",
    "При этом все эти операции обобщены так, чтобы их можно было выполнять с коэффициентами от 0 до 1\n",
    "* 0 - операция не выполняется\n",
    "* 1 - операция выполняется в полной мере\n",
    "* что-то между - операция выполняется отчасти\n",
    "* При этом  \n",
    "  * `0 <= push, pop, no-op <= 1`\n",
    "  * `push + pop + no-op = 1`\n",
    "\n",
    "В итоге обновление стэка выглядит так\n",
    "\n",
    "```Stack(depth i, t+1) = push * Stack(depth i+1, t) + pop * Stack(depth i-1, t) + no-op * Stack(depth i, t)```\n",
    "\n",
    "\n",
    "* В качестве \"вводимого\" элемента для push можно использовать элемент, полученный из скрытого состояния сети\n",
    "* При выполнении pop, элемент стэка на максимальной глубине заполняется нулевыми значениями с соответствующим коэффициентом\n",
    "* Самое первое (depth 0) значение в стэке влияет на новое значение рекуррентного слоя\n",
    "\n",
    "\n",
    "\n",
    "Как это всё делать:\n",
    " * Создадим слой, реализующий стэковую память\n",
    " * Воткнём его в сеть\n",
    " * ???\n",
    " * PROFIT!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стэковая память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers.base import MergeLayer\n",
    "\n",
    "class StackAugmentation(MergeLayer):\n",
    "    def __init__(self,\n",
    "                 observation_input,\n",
    "                 prev_state_input,\n",
    "                 controls_layer,\n",
    "                 **kwargs):\n",
    "\n",
    "        \n",
    "        #default name\n",
    "        if \"name\" not in kwargs:\n",
    "            kwargs[\"name\"] = \"YetAnother\"+self.__class__.__name__\n",
    "        \n",
    "               \n",
    "        super(StackAugmentation, self).__init__([observation_input,prev_state_input,controls_layer], **kwargs)\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "            Нам на вход приходят\n",
    "             - вводимый элемент формы [None, ширина стэка]\n",
    "             - состояние стэка на предыдущем шаге, форма [None,глубина стэка, ширина стэка]\n",
    "             - вектор управления стэком, форма [None, 3] - push, pop и no-op соответственно\n",
    "             \n",
    "            На выход мы хотим новое состояние стэка\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #кто они\n",
    "        input_val,prev_stack,controls = inputs\n",
    "        assert input_val.ndim==2\n",
    "        \n",
    "        \n",
    "        #немного преобразований формы (чтобы вам не нужно было делать это самосттоятельно)\n",
    "        controls = controls.reshape([-1,3,1,1])    \n",
    "        input_val = input_val[:,None,:]\n",
    "        zeros_at_the_top = T.zeros_like(prev_stack[:,0,None,:])\n",
    "        \n",
    "        # операции управления стэком\n",
    "        a_push,a_pop,a_no_op = controls[:,0],controls[:,1],controls[:,2]\n",
    "        \n",
    "        \n",
    "        # Промежуточный этап - подготовим версии стэка, сдвинутые на единицу вверх или вниз.\n",
    "        # Дописывать крайние элементы проще всего через T.concatenate(axis=1) или T.horizontal_stack\n",
    "        \n",
    "        \n",
    "        \n",
    "        stack_popped = стэк, сдвинутый вниз на 1 единицу глубины(начало выброшено), в конец которого дописан zeros_at_the_top\n",
    "        \n",
    "        \n",
    "        stack_pushed = стэк, сдвинутый вверх на 1 единицу глубины (конец выброшен), в начало которого дописан input_val\n",
    "        \n",
    "        \n",
    "        new_stack = формула для нового стэка с использованием 3 сигналов управления и соответствующих вариаций стэка\n",
    "\n",
    "        return new_stack\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        \"\"\"\n",
    "        Ещё 1 обязательный для lasagne (но не для Вас) метод слоя - вычисление размерности выхода по размерностям входов\n",
    "        \"\"\"\n",
    "        observation_shape,last_memory_state_shape,controls_shape = input_shapes\n",
    "        \n",
    "        return last_memory_state_shape\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Струтктурная схема\n",
    "![canvas](stack-rnn.png)\n",
    "\n",
    "\n",
    "* А теперь вылезай из-под стола. На самом деле всё проще.\n",
    "* Всё, что не обведено жЫрной чОрной линией - уже было в предыдущей схеме\n",
    "* Prev stack / Next stack - предыдущее и новое состояния стэка\n",
    "* StackAugmentation - слой, который вы только что реализовали\n",
    "* Stack Input и Controls - просто Dense слои\n",
    "* First - это операция подсматривания в верхний элемент стака (SliceLayer)\n",
    "\n",
    "Для вашей простоты, RNN часть уже сделана по аналогии с предыдущей сетью \n",
    "* (если вам больше нравится ваша имплементация - просто скопируйте её)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#observation - сюда на каждом шаге подаётся очередная буква из входной последовательности\n",
    "output_shape = (None,)\n",
    "observation_layer = InputLayer(output_shape,name=\"obs_input\")\n",
    "\n",
    "\n",
    "# Token embedding\n",
    "n_tokens = len(alphabet)\n",
    "obs_embedding = EmbeddingLayer(observation_layer,\n",
    "                                              input_size=n_tokens,\n",
    "                                              output_size=n_tokens,\n",
    "                                              name = \"input_embedding\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#число нейронов в скрытом слое RNN - чуть меньше, чтобы сумма всех весов получилась такой же, как у RNN без стэка\n",
    "n_hid_1 = 64\n",
    "\n",
    "#Сюда прицепится предыдущее состояние RNN\n",
    "prev_rnn_layer = InputLayer((None,n_hid_1),name=\"prev_rnn_state\")\n",
    "\n",
    "\n",
    "# Сюда прицепится предыдущее стостояние стэка\n",
    "stack_width = 3\n",
    "stack_depth = 50\n",
    "\n",
    "prev_stack_layer = InputLayer((None,stack_depth,stack_width))\n",
    "\n",
    "\n",
    "\n",
    "# Controls\n",
    "stack_controls_layer = Слой управления стаком - принимает предыдущее состояние rnn и генерит 3 выхода, которые суммируются в 1 (softmax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Stack input\n",
    "stack_input_layer = Полносвязный слой от того же входа, размером в 3 нейрона. В качестве нелинейности что угодно (tanh например)\n",
    "    \n",
    "    \n",
    "#новое состояние стэка  - используем вашу функцию обновления\n",
    "next_stack = StackAugmentation(stack_input_layer,\n",
    "                              prev_stack_layer,\n",
    "                              stack_controls_layer)\n",
    "\n",
    "\n",
    "#возьмём первый элемент стэка (First), чтобы использовать в RNN\n",
    "stack_top = lasagne.layers.SliceLayer(next_stack,0,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RNN memory\n",
    "\n",
    "\n",
    "#RNN from input\n",
    "rnn_frominput = DenseLayer(obs_embedding,\n",
    "                           num_units=n_hid_1,\n",
    "                           name= \"rnn input to hidden\",\n",
    "                           b= None,\n",
    "                           nonlinearity = None)\n",
    "\n",
    "\n",
    "#RNN from hidden\n",
    "\n",
    "rnn_fromhidden = DenseLayer(prev_rnn_layer,\n",
    "                            num_units=n_hid_1,\n",
    "                            name= \"rnn hidden to hidden\",\n",
    "                            nonlinearity = None)\n",
    "                            \n",
    "#RNN from stack\n",
    "rnn_fromstack = Обновление RNN на основании stack_top\n",
    "\n",
    "\n",
    "rnn_sum = ElemwiseSumLayer([\n",
    "        rnn_frominput,\n",
    "        rnn_fromhidden,\n",
    "        rnn_fromstack     \n",
    "    ],\n",
    "    name = \"rnn_sum\")\n",
    "#между прочим это RNN с тремя входами, один из которых зависит от её предыдущего состояние,\n",
    "#и лазанья такое уже не умеет\n",
    "\n",
    "rnn = NonlinearityLayer(rnn_sum,lasagne.nonlinearities.tanh,\n",
    "                        name = \"rnn nonlinearity\")\n",
    "\n",
    "\n",
    "\n",
    "# В этом словаре лежат пары  {выход скрытого состояния: вход скрытого состояния с предыдущего шага}\n",
    "# допишем сюда стэк\n",
    "from collections import OrderedDict\n",
    "memory_dict = OrderedDict([\n",
    "            (rnn,prev_rnn_layer),\n",
    "            (next_stack, prev_stack_layer)\n",
    "    ])\n",
    "\n",
    "\n",
    "#Вероятности букв\n",
    "\n",
    "probability_layer = lasagne.layers.DenseLayer(rnn,\n",
    "                                         num_units = n_tokens,\n",
    "                                         nonlinearity=  lasagne.nonlinearities.softmax,\n",
    "                                         name=\"policy_original\")\n",
    "\n",
    "#resolver - выбирает конкретную букву пропорционально вероятностям\n",
    "\n",
    "resolver = ProbablisticResolver(probability_layer,\n",
    "                                assume_normalized=True,\n",
    "                                name=\"resolver\")\n",
    "\n",
    "\n",
    "#Проверка, совпадают ли формы у состояний. Должна пройти, если вы не сделали чего-то ужасного\n",
    "assert tuple(lasagne.layers.get_output_shape(resolver)) == tuple(output_shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Создаём рекуррентный генератор, который\n",
    "# - имеет 1 вход - observation layer\n",
    "# - имеет память, описанную в memory dict ( один слой RNN )\n",
    "# - генерирует буквы с вероятностями, полученными на probability layer\n",
    "# - принимает решение, какую букву генерить, на слое resolver layer - пропорционально вероятностям\n",
    "\n",
    "agent = Generator(\n",
    "    observation_layer,\n",
    "    memory_dict,\n",
    "    probability_layer,\n",
    "    resolver\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Получим веса - проследите, чтобы там были веса всех обучаемых частей стэка\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_weights = int(T.sum([T.prod(w.shape) for w in weights]).eval())\n",
    "print \"Всего весов:\", total_weights\n",
    "\n",
    "#если вы экспериментируете с размером нейронки - удалите следующую строку\n",
    "assert 5000 < total_weights <= 5500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy-paste time\n",
    "\n",
    "Ниже следует абсолютно тот же код, что и в случае с RNN \n",
    "- желающие могут проследить \n",
    "- только без комментариев и в одной массе.\n",
    "- почему для этого не созданы функции - чтобы было проще отлаживать\n",
    "- __от вас требуется только скопировать туда строчку с оптимизатором__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_seq = agent.get_sessions(session_length=SEQ_LENGTH,\n",
    "                             recorded_sequences=sequences_batch,\n",
    "                             batch_size=batch_size,)[-1]\n",
    "\n",
    "model_loss = lasagne.objectives.categorical_crossentropy(\n",
    "    T.maximum(probas_seq[:,:-1],1e-10).reshape([-1,n_tokens]),\n",
    "    sequences_batch[:,1:].ravel()\n",
    ").mean()\n",
    "\n",
    "loss = model_loss + regularize_network_params(resolver,l2)*10**-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "updates = ваш любивый оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([sequences_batch],[loss],updates=updates)\n",
    "evaluation_fun = theano.function([sequences_batch],[loss,model_loss,reg_l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_batch_size = T.scalar('generated batch size','int32')\n",
    "generated_action_seq = agent.get_sessions(session_length=SEQ_LENGTH,\n",
    "                             batch_size=gen_batch_size,)[-2]\n",
    "get_sequences = theano.function([gen_batch_size],generated_action_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Цикл обучения - Stack-augmented RNN\n",
    "\n",
    "* Всё как обычно - учимся и раз в 100 итераций считаем метрики\n",
    "\n",
    "* Метрики нас интересуют 3 - \n",
    " * llh - простой loglikelihood - должен падать +- шум\n",
    " * доля правильных последовательностей - с какой вероятностью rnn сгенерит подряд сколько-то a, потом b, потом c - и потом начнёт новую строку.\n",
    "   * для формалистов - доля строк, которые полностью матчатся регээкспом \"^|a+b+c+\"\n",
    "   * тоже должен в целом расти, пока не выйдёт на асимптоту близь единицы\n",
    " * ошибка в количестве C\n",
    "   * рассматриваем только \"правильные\" последовательности по определению из предыдущего пункта\n",
    "   * считаем количество букв a (n), количество букв b (m). Правильное количество букв с - n+m\n",
    "   * Считаем средний модуль разности между сгенерированным количеством С и правильным.\n",
    "   * Формально метрика называется mean absolute error или MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "metrics = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(N_EPOCHS):\n",
    "    \n",
    "    #породим строки\n",
    "    new_batch = generate_sequence(BATCH_SIZE,SEQ_LENGTH)\n",
    "    #Потренируем чудовище\n",
    "    train_fun(new_batch)\n",
    "    \n",
    "    #Once in a while, напечатаем метрики\n",
    "    if i % REPORT_RATE==0:\n",
    "        \n",
    "        loss_components = evaluation_fun(new_batch)\n",
    "        print \"iter:%i\\tfull:%.5f\\tllh:%.5f\\treg:%.5f\"%tuple([i]+map(float,loss_components))        \n",
    "        \n",
    "        metrics['crossentropy'][i]=float(loss_components[1])\n",
    "        \n",
    "\n",
    "        examples = get_sequences(1000)\n",
    "        \n",
    "        correctness_ratio,c_count_mae = get_metrics(examples,alphabet)\n",
    "        \n",
    "\n",
    "        metrics[\"correctness_rates\"][i] = correctness_ratio\n",
    "        metrics[\"c_count_errors\"][i]=c_count_mae\n",
    "        \n",
    "        print \"Доля последовательностей правильной формы: %.5f\"%(correctness_ratio)\n",
    "        print \"MAE Ошибка по количеству C среди правильных: %.5f\"%(c_count_mae)\n",
    "        \n",
    "        for tid_line in examples[:3]:\n",
    "            print ' '.join(map(alphabet.__getitem__,tid_line))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_5_last = lambda metric_name: map(lambda v: v[1],sorted(metrics[metric_name].items(),key=lambda v:v[0])[-5:])\n",
    "\n",
    "\n",
    "#проверяем, что минимум по последним 5 значениям метрики не хуже порога\n",
    "\n",
    "assert min(get_5_last(\"crossentropy\")) <= 0.25\n",
    "assert min(get_5_last(\"c_count_errors\")) <= 1\n",
    "assert min(get_5_last(\"correctness_rates\")) >= 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кривулины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    plt.figure(figsize=[14,8])\n",
    "    plt.plot(*zip(*sorted(metrics[metric].items(),key=lambda (k,v):k)),label='Stack RNN '+metric)\n",
    "    plt.plot(*zip(*sorted(rnn_metrics[metric].items(),key=lambda (k,v):k)),label='Simple RNN '+metric)\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel(\"popugai\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Вопросы на CPU-time\n",
    "\n",
    "* Как изменится (и изменится ли) картина, если уменьшить/увеличить\n",
    " * количество нейронов в RNN в 2 раза больше/меньше как со стэком, так и без?\n",
    " * размер стэка (stack width) и его глубину? Какая минмальная глубина нужна, чтобы был видимый эффект?\n",
    " * сделать ещё 1+ стэк?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кто победил?\n",
    "\n",
    "Ну конечно же, <так кто же победил?>, однако <комментарии>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
