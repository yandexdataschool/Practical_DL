
## Materials
* Adaptive optimization methods (russian) - [video](https://yadi.sk/i/SAGl44PS3EHZeK)
* Stochastic gradient descent modiffications (english) - [video](https://www.youtube.com/watch?v=nhqo0u1a6fw)
* A blog post overview of gradient descent methods - [url](http://ruder.io/optimizing-gradient-descent/)

## More materials
* [Cool interactive demo of momentum](http://distill.pub/2017/momentum/)
* [wikipedia on SGD :)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), expecially the "extensions and variants" section
* [RMSPROP video](https://www.youtube.com/watch?v=defQQqkXEfE)

## Practice

As usual, go to the only notebook in this folder (`adaptive_sgd.ipynb`) and follow instructions from there.

__Note__: Starting from next seminar, assignments will require you to install a deep learning framework. Click [here](https://github.com/yandexdataschool/Practical_DL/issues/6) for details.
