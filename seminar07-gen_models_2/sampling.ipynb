{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = 'Colab' # 'YC', 'Colab'\n",
    "\n",
    "if backend == 'Colab':\n",
    "    !sudo apt install -y ninja-build\n",
    "    !git clone https://github.com/yandexdataschool/Practical_DL.git\n",
    "    !wget https://www.dropbox.com/s/2kpsomtla61gjrn/pretrained.tar\n",
    "    !tar -xvf pretrained.tar\n",
    "elif backend == 'YC':\n",
    "    # Yandex Cloud (temporary unavailable)\n",
    "    %wget https://www.dropbox.com/s/2kpsomtla61gjrn/pretrained.tar\n",
    "    %tar -xvf pretrained.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm.auto import tqdm, trange\n",
    "from PIL import Image\n",
    "\n",
    "from gans.gan_load import make_stylegan2, make_big_gan\n",
    "\n",
    "\n",
    "def to_image(tensor, adaptive=False):\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor[0]\n",
    "    if adaptive:\n",
    "        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    else:\n",
    "        tensor = ((tensor + 1) / 2).clamp(0, 1)\n",
    "\n",
    "    return ToPILImage()((255 * tensor.cpu().detach()).to(torch.uint8))\n",
    "\n",
    "\n",
    "def to_image_grid(tensor, adaptive=False, **kwargs):\n",
    "    return to_image(make_grid(tensor, **kwargs), adaptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTA GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L\n",
    "G = make_big_gan('pretrained/G_ema.pth', 128).cuda().eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([2, 512]).cuda()\n",
    "    classes = nn.Parameter(torch.tensor([239, 100], dtype=torch.int64), requires_grad=False).cuda()\n",
    "    \n",
    "    cl_embed = G.big_gan.shared(classes)\n",
    "    cl_embed_swap = torch.stack([cl_embed[1], cl_embed[0]])\n",
    "\n",
    "    imgs = G.big_gan(z, cl_embed)\n",
    "    imgs_cl_swap = G.big_gan(z, cl_embed_swap)\n",
    "\n",
    "    interps = torch.arange(0, 1.01, 0.2).cuda()\n",
    "    embeds = torch.stack([torch.lerp(cl_embed[0], cl_embed[1], a) for a in interps])\n",
    "\n",
    "    imgs_cl_interp = G.big_gan(z[0][None].repeat(len(embeds), 1), embeds)\n",
    "    \n",
    "_, axs = plt.subplots(3, 1, dpi=200)\n",
    "for ax in axs: ax.axis('off')\n",
    "\n",
    "axs[0].imshow(to_image_grid(imgs))\n",
    "axs[1].imshow(to_image_grid(imgs_cl_swap))\n",
    "axs[2].imshow(to_image_grid(imgs_cl_interp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L\n",
    "G = make_stylegan2(resolution=1024,\n",
    "                   weights='pretrained/stylegan2-ffhq-config-f.pt').eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([4, 512]).cuda()\n",
    "    imgs = G(z)\n",
    "\n",
    "    z = torch.stack([a * z[0] + (1 - a) * z[1] for a in torch.arange(0, 1.01, 0.2)])\n",
    "    imgs_interp = G(z)\n",
    "\n",
    "_, axs = plt.subplots(2, 1, dpi=200)\n",
    "for ax in axs: ax.axis('off')\n",
    "\n",
    "axs[0].imshow(to_image_grid(imgs, nrow=6))\n",
    "axs[1].imshow(to_image_grid(imgs_interp, nrow=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L\n",
    "from supervised_deformation_finder.model import CelebaAttributeClassifier\n",
    "from supervised_deformation_finder.utils import \\\n",
    "    prepare_generator_output_for_celeba_regressor as preprocess\n",
    "\n",
    "\n",
    "print('- Attributes -\\n')\n",
    "with open('supervised_deformation_finder/celeba_attributes.txt') as f:\n",
    "    attributes = f.readline().split(' ')\n",
    "    attributes.sort()\n",
    "    for i, att in enumerate(attributes):\n",
    "        if i % 4 != 0:\n",
    "            print(att.ljust(22), end='')\n",
    "        else:\n",
    "            print(att)\n",
    "\n",
    "\n",
    "regressor = CelebaAttributeClassifier('Smiling', 'pretrained/regressor.pth').cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L\n",
    "from dataclasses import dataclass\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ShiftedGSample:\n",
    "    latent: list = None\n",
    "    val: float = None\n",
    "\n",
    "\n",
    "def train_normal(samples, svm_max_iter=10_000):\n",
    "    num_samples = len(samples)\n",
    "    latents = torch.stack([s.latent for s in samples])\n",
    "    expectations = torch.tensor([s.val for s in samples])\n",
    "\n",
    "    # estimate attribute direction\n",
    "    print(f'Starting SVM training with {num_samples} samples')\n",
    "    svm = LinearSVR(max_iter=svm_max_iter)\n",
    "    svm.fit(latents, expectations)\n",
    "    normal = torch.from_numpy(svm.coef_).to(torch.float).cuda()\n",
    "\n",
    "    return normal\n",
    "\n",
    "\n",
    "# accumulate statistics\n",
    "num_steps, batch = 200, 8\n",
    "samples = []\n",
    "for latents in tqdm(torch.randn([num_steps, batch, 512])):\n",
    "    with torch.no_grad():\n",
    "        latents = G.style_gan2.style(latents.cuda())\n",
    "        imgs = G(latents, w_space=True)\n",
    "        probs = regressor.get_probs(preprocess(imgs))[:, 1]\n",
    "    samples += [ShiftedGSample(l, p) for l, p in zip(latents.cpu(), probs.cpu())]\n",
    "\n",
    "for i, ax in enumerate(plt.subplots(1, len(imgs), dpi=250)[1]):\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{probs[i].item(): 0.2f}', fontdict=dict(size=7))\n",
    "    ax.imshow(to_image(imgs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L\n",
    "shift = train_normal(samples, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([8, 512]).cuda()\n",
    "    imgs_orig = G(z)\n",
    "    \n",
    "    w = G.style_gan2.style(z)\n",
    "    imgs_shifted = G(w - 3 * shift, w_space=True)\n",
    "\n",
    "to_image_grid(torch.cat([imgs_orig, imgs_shifted]), nrow=len(imgs_orig))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
