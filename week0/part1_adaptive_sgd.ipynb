{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 0 (Linear models, Optimization)\n",
    "\n",
    "In this seminar you will implement a linear classifier and train it using stochastic gradient descent modiffications, numpy and your brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load our dakka\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-dimensional classification\n",
    "\n",
    "To make things more intuitive, let's solve a 2D classification problem with syntetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "(X, y) = datasets.make_circles(n_samples=1024, shuffle=True, noise=0.2, factor=0.4)\n",
    "ind = np.logical_or(y==1, X[:,1] > X[:,0] - 0.5)\n",
    "X = X[ind,:]\n",
    "m = np.array([[1, 1], [-2, 1]])\n",
    "X = preprocessing.scale(X)\n",
    "y = y[ind]\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"X:\\n{}\\ny:\\n{}\".format(X[:3],y[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task starts here**\n",
    "\n",
    "Since the problem above isn't linearly separable, we add quadratic features to the classifier.\n",
    "\n",
    "Implement this transformation in the __expand__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand(X):\n",
    "    \"\"\"\n",
    "    Adds quadratic features. \n",
    "    This function allows your linear model to make non-linear separation.\n",
    "    \n",
    "    For each sample (row in matrix), compute an expanded row:\n",
    "    [feature0, feature1, feature0^2, feature1^2, feature1*feature2, 1]\n",
    "    \n",
    "    :param X: matrix of features, shape [n_samples,2]\n",
    "    :returns: expanded features of shape [n_samples,6]\n",
    "    \"\"\"\n",
    "    X_expanded = np.zeros((X.shape[0], 6))\n",
    "    \n",
    "    <your code here>\n",
    "    \n",
    "    return X_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple test on random numbers\n",
    "#[all 8 random numbers are 100% random :P]\n",
    "dummy_X = np.array([\n",
    "        [0,0],\n",
    "        [1,0],\n",
    "        [2.61,-1.28],\n",
    "        [-0.59,2.1]\n",
    "    ])\n",
    "\n",
    "#call your expand function\n",
    "dummy_expanded = expand(dummy_X)\n",
    "\n",
    "#what it should have returned:   x0       x1       x0^2     x1^2     x0*x1    1\n",
    "dummy_expanded_ans = np.array([[ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  1.    ],\n",
    "                               [ 1.    ,  0.    ,  1.    ,  0.    ,  0.    ,  1.    ],\n",
    "                               [ 2.61  , -1.28  ,  6.8121,  1.6384, -3.3408,  1.    ],\n",
    "                               [-0.59  ,  2.1   ,  0.3481,  4.41  , -1.239 ,  1.    ]])\n",
    "\n",
    "#tests\n",
    "assert isinstance(dummy_expanded,np.ndarray), \"please make sure you return numpy array\"\n",
    "assert dummy_expanded.shape==dummy_expanded_ans.shape, \"please make sure your shape is correct\"\n",
    "assert np.allclose(dummy_expanded,dummy_expanded_ans,1e-3), \"Something's out of order with features\"\n",
    "\n",
    "print(\"Seems legit!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write function that predicts class given X as in logistic regression.\n",
    "\n",
    "The math should look like this:\n",
    "\n",
    "$$ P(y| \\vec x, \\vec w) = \\sigma(\\vec x \\cdot \\vec w )$$\n",
    "\n",
    "where x represents features, w are weights and $$\\sigma(a) = {1 \\over {1+e^{-a}}}$$\n",
    "\n",
    "We shall omit $ \\vec {arrows} $ in further formulae for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(X, w):\n",
    "    \"\"\"\n",
    "    Given input features and weights\n",
    "    return predicted probabilities of y==1 given x, P(y=1|x), see description above\n",
    "        \n",
    "    __don't forget to expand X inside classify and other functions__\n",
    "    \n",
    "    :param X: feature matrix X of shape [n_samples,2] (non-exanded)\n",
    "    :param w: weight vector w of shape [6] for each of the expanded features\n",
    "    :returns: an array of predicted probabilities in [0,1] interval.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample usage / test just as the previous one\n",
    "dummy_weights = np.linspace(-1,1,6)\n",
    "\n",
    "dummy_probs = classify(dummy_X,dummy_weights)\n",
    "\n",
    "dummy_answers = np.array([ 0.73105858,  0.450166  ,  0.02020883,  0.59844257])\n",
    "\n",
    "assert isinstance(dummy_probs,np.ndarray), \"please return np.array\"\n",
    "assert dummy_probs.shape == dummy_answers.shape, \"please return an 1-d vector with answers for each object\"\n",
    "assert np.allclose(dummy_probs,dummy_answers,1e-3), \"There's something non-canonic about how probabilties are computed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss you should try to minimize is the Logistic Loss aka crossentropy aka negative log-likelihood:\n",
    "\n",
    "$$ L =  - {1 \\over N} \\sum_i {y \\cdot log P(y|x,w) + (1-y) \\cdot log (1-P(y|x,w))}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,2], target vector [n_samples] of +1/-1,\n",
    "    and weight vector w [6], compute scalar loss function using formula above.\n",
    "    \"\"\"\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_y = np.array([0,1,0,1])\n",
    "dummy_loss = compute_loss(dummy_X,dummy_y,dummy_weights)\n",
    "\n",
    "assert np.allclose(dummy_loss,0.66131), \"something wrong with loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we train our model with gradient descent, we gotta compute gradients.\n",
    "\n",
    "To be specific, we need a derivative of loss function over each weight [6 of them].\n",
    "\n",
    "$$ \\nabla L = {\\partial L \\over \\partial w} = ...$$\n",
    "\n",
    "No, we won't be giving you the exact formula this time. Instead, try figuring out a derivative with pen and paper. \n",
    "\n",
    "As usual, we've made a small test for you, but if you need more, feel free to check your math against finite differences (estimate how L changes if you shift w by $10^-5$ or so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_grad(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,2], target vector [n_samples] of +1/-1,\n",
    "    and weight vector w [6], compute vector [6] of derivatives of L over each weights.\n",
    "    \"\"\"\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tests\n",
    "dummy_grads = compute_grad(dummy_X,dummy_y,dummy_weights)\n",
    "\n",
    "#correct answers in canonic form\n",
    "dummy_grads_ans = np.array([-0.06504252, -0.21728448, -0.1379879 , -0.43443953,  0.107504  , -0.05003101])\n",
    "\n",
    "assert isinstance(dummy_grads,np.ndarray)\n",
    "assert dummy_grads.shape == (6,), \"must return a vector of gradients for each weight\"\n",
    "assert len(set(np.round(dummy_grads/dummy_grads_ans,3))), \"gradients are wrong\"\n",
    "assert np.allclose(dummy_grads,dummy_grads_ans,1e-3), \"gradients are off by a coefficient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an auxiliary function that visualizes the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "def visualize(X, y, w, history):\n",
    "    \"\"\"draws classifier prediction with matplotlib magic\"\"\"\n",
    "    Z = classify(np.c_[xx.ravel(), yy.ravel()], w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history)\n",
    "    plt.grid()\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.ylim(0, ymax)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualize(X,y,dummy_weights,[1,0.5,0.25],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In this section, we'll use the functions you wrote to train our classifier using stochastic gradient descent.\n",
    "\n",
    "Try to find an optimal learning rate for gradient descent for the given batch size. \n",
    "\n",
    "**Don't change the batch size!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([0,0,0,0,0,1])\n",
    "\n",
    "\n",
    "alpha = <learning rate>\n",
    "\n",
    "n_iter = 50\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X, y, w)\n",
    "    visualize(X[ind,:], y[ind], w, loss)\n",
    "    \n",
    "    w = w - alpha * compute_grad(X[ind,:], y[ind], w)\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent with momentum and test it's performance for different learning rate and momentum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([0,0,0,0,0,1])\n",
    "\n",
    "alpha = 0.0 # learning rate\n",
    "mu    = 0.0 # momentum\n",
    "\n",
    "n_iter = 50\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X, y, w)\n",
    "    visualize(X[ind,:], y[ind], w, loss)\n",
    "    \n",
    "    <update w and anything else here>\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement RMSPROP algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([0,0,0,0,0,1])\n",
    "\n",
    "alpha = 0.0 # learning rate\n",
    "mean_squared_norm = 0.0 #moving average of gradient norm squared\n",
    "\n",
    "n_iter = 50\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X, y, w)\n",
    "    visualize(X[ind,:], y[ind], w, loss, n_iter)\n",
    "    \n",
    "    <update w and anything else here>\n",
    "\n",
    "\n",
    "visualize(X, y, w, loss, n_iter)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which optimization method you consider the best? Type your answer in the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus quest \n",
    "try the same thing for Adagrad, Adam and anything else you find suitable.\n",
    "\n",
    "_Each new adaptive optimizer is worth 2 points!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
