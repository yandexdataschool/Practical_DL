{"nbformat":4,"nbformat_minor":4,"metadata":{"accelerator":"GPU","colab":{"name":"seminar_pytorch.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7"},"notebookId":"0bd81ca7-4175-4905-a84c-21ed8da72299"},"cells":[{"cell_type":"markdown","source":"# Deep learning for computer vision\n\n\nThis notebook will teach you to build and train convolutional networks for image recognition. Brace yourselves.","metadata":{"id":"71AQJg3CDMn9","cellId":"hsbyb4tyki9nx32utdtjpk"}},{"cell_type":"markdown","source":"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/Practical_DL/blob/spring20/seminar3/seminar3_pytorch.ipynb)","metadata":{"id":"LaGKim3ykLXp","cellId":"rxq6pql5x4tle9o386ep"}},{"cell_type":"markdown","source":"# Tiny ImageNet dataset\nThis week, we shall focus on the image recognition problem on Tiny Image Net dataset\n* 100k images of shape 3x64x64\n* 200 different classes: snakes, spaiders, cats, trucks, grasshopper, gull, etc.\n","metadata":{"id":"2MaELIpIDMoA","cellId":"34l2kkk7t84llsmzyxus1"}},{"cell_type":"code","source":"#!L\nimport torchvision\nimport torch\nfrom torchvision import transforms","metadata":{"id":"rS_-00tYDMoB","cellId":"g2i37mixtk9kkxkki1y8","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#!S:bash\nwget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UksGhGn63aQLAfGrAkGzdx69U6waEHPR' -O tinyim3.png\nwget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19qsD0o7pfAI8UYxgDY18sdRjV0Aantn2' -O tiny_img.py\nwget --no-check-certificate 'https://docs.google.com/uc?export=download&id=12IrLjz8pss4284xsBAJt6CW6yELPH4tL' -O tiniim.png","metadata":{"id":"sCvh1ICbHNCE","cellId":"k1eayz1ur2mqly9zrk5my","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"--2021-03-02 15:50:42--  https://docs.google.com/uc?export=download&id=1UksGhGn63aQLAfGrAkGzdx69U6waEHPR\nResolving docs.google.com (docs.google.com)... 64.233.164.102, 64.233.164.138, 64.233.164.100, ...\nConnecting to docs.google.com (docs.google.com)|64.233.164.102|:443... connected.\nHTTP request sent, awaiting response... 302 Moved Temporarily\nLocation: https://doc-0k-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8bnls7ujdb88bq8ivp3qe4ja4q6q961j/1614700200000/01961971800886548445/*/1UksGhGn63aQLAfGrAkGzdx69U6waEHPR?e=download [following]\nWarning: wildcards not supported in HTTP.\n--2021-03-02 15:50:43--  https://doc-0k-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8bnls7ujdb88bq8ivp3qe4ja4q6q961j/1614700200000/01961971800886548445/*/1UksGhGn63aQLAfGrAkGzdx69U6waEHPR?e=download\nResolving doc-0k-6s-docs.googleusercontent.com (doc-0k-6s-docs.googleusercontent.com)... 108.177.14.132, 2a00:1450:4010:c09::84\nConnecting to doc-0k-6s-docs.googleusercontent.com (doc-0k-6s-docs.googleusercontent.com)|108.177.14.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 52313 (51K) [image/png]\nSaving to: ‘tinyim3.png’\n\n     0K .......... .......... .......... .......... .......... 97% 1.26M 0s\n    50K .                                                     100% 14.3M=0.04s\n\n2021-03-02 15:50:43 (1.29 MB/s) - ‘tinyim3.png’ saved [52313/52313]\n\n--2021-03-02 15:50:43--  https://docs.google.com/uc?export=download&id=19qsD0o7pfAI8UYxgDY18sdRjV0Aantn2\nResolving docs.google.com (docs.google.com)... 64.233.164.139, 64.233.164.113, 64.233.164.100, ...\nConnecting to docs.google.com (docs.google.com)|64.233.164.139|:443... connected.\nHTTP request sent, awaiting response... 302 Moved Temporarily\nLocation: https://doc-0c-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q4opja7ilag1kpgmfahku6qsbr316etg/1614700200000/01961971800886548445/*/19qsD0o7pfAI8UYxgDY18sdRjV0Aantn2?e=download [following]\nWarning: wildcards not supported in HTTP.\n--2021-03-02 15:50:44--  https://doc-0c-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/q4opja7ilag1kpgmfahku6qsbr316etg/1614700200000/01961971800886548445/*/19qsD0o7pfAI8UYxgDY18sdRjV0Aantn2?e=download\nResolving doc-0c-6s-docs.googleusercontent.com (doc-0c-6s-docs.googleusercontent.com)... 209.85.233.132, 2a00:1450:4010:c07::84\nConnecting to doc-0c-6s-docs.googleusercontent.com (doc-0c-6s-docs.googleusercontent.com)|209.85.233.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3378 (3.3K) [text/x-python-script]\nSaving to: ‘tiny_img.py’\n\n     0K ...                                                   100%  204M=0s\n\n2021-03-02 15:50:44 (204 MB/s) - ‘tiny_img.py’ saved [3378/3378]\n\n--2021-03-02 15:50:44--  https://docs.google.com/uc?export=download&id=12IrLjz8pss4284xsBAJt6CW6yELPH4tL\nResolving docs.google.com (docs.google.com)... 64.233.162.102, 64.233.162.113, 64.233.162.139, ...\nConnecting to docs.google.com (docs.google.com)|64.233.162.102|:443... connected.\nHTTP request sent, awaiting response... 302 Moved Temporarily\nLocation: https://doc-00-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/eaq86c577psmcshug630gemj3afn51bg/1614700200000/01961971800886548445/*/12IrLjz8pss4284xsBAJt6CW6yELPH4tL?e=download [following]\nWarning: wildcards not supported in HTTP.\n--2021-03-02 15:50:45--  https://doc-00-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/eaq86c577psmcshug630gemj3afn51bg/1614700200000/01961971800886548445/*/12IrLjz8pss4284xsBAJt6CW6yELPH4tL?e=download\nResolving doc-00-6s-docs.googleusercontent.com (doc-00-6s-docs.googleusercontent.com)... 64.233.164.132, 2a00:1450:4010:c0f::84\nConnecting to doc-00-6s-docs.googleusercontent.com (doc-00-6s-docs.googleusercontent.com)|64.233.164.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 59018 (58K) [image/png]\nSaving to: ‘tiniim.png’\n\n     0K .......... .......... .......... .......... .......... 86% 1.39M 0s\n    50K .......                                               100% 3.49M=0.04s\n\n2021-03-02 15:50:45 (1.51 MB/s) - ‘tiniim.png’ saved [59018/59018]\n\n"}],"execution_count":10},{"cell_type":"code","source":"#!L\nfrom tiny_img import download_tinyImg200\ndata_path = '.'\ndownload_tinyImg200(data_path)","metadata":{"id":"5rQhiYyRDMoG","cellId":"5nh892g5zpl9qv5fki8vpk","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"./tiny-imagenet-200.zip\n"}],"execution_count":11},{"cell_type":"code","source":"#!L\ndataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=transforms.ToTensor())\ntest_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/val', transform=transforms.ToTensor())\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000])\ntest_dataset, val_dataset = torch.utils.data.random_split(val_dataset, [10000, 10000])","metadata":{"id":"5vq5Cm0ADMoK","cellId":"jrzsbgniodgtg1hif324k9","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 100000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 100000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 20000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 10000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 10000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 80000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n"}],"execution_count":12},{"cell_type":"code","source":"#!L\nbatch_size = 50\ntrain_batch_gen = torch.utils.data.DataLoader(train_dataset, \n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=1)","metadata":{"id":"tY6OUeOODMoN","cellId":"6md8io0fesfby4r9per3jb","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 100000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 100000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 20000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 10000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 10000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 80000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n"}],"execution_count":13},{"cell_type":"code","source":"#!L\nval_batch_gen = torch.utils.data.DataLoader(val_dataset, \n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=1)","metadata":{"id":"HBgW-gzwDMoQ","cellId":"hsq566ut87vokpkiq68","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 100000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 100000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 20000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 10000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 10000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 80000\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n"}],"execution_count":14},{"cell_type":"markdown","source":"## Image examples ##","metadata":{"id":"swKtJaVyDMoU","cellId":"rjwf5t0s4f8zbnfmyu7q"}},{"cell_type":"markdown","source":"\n\n<tr>\n    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tinyim3.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tinyim2.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n</tr>\n","metadata":{"id":"h5wImXEaDMoV","cellId":"nbxuu26h8hhcgzzgeh5nh"}},{"cell_type":"markdown","source":"<tr>\n    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tiniim.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n</tr>","metadata":{"id":"Do-qRQp8DMoW","cellId":"w71ngep3y8jm2s3xt0qg"}},{"cell_type":"markdown","source":"# Building a network\n\nSimple neural networks with layers applied on top of one another can be implemented as `torch.nn.Sequential` - just add a list of pre-built modules and let it train.","metadata":{"id":"arxSyhBLDMoX","cellId":"fxzxgbl11g2dixss4t9nx"}},{"cell_type":"code","source":"#!L\nimport torch, torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n","metadata":{"id":"7QF2hMVxDMoY","cellId":"g5yf9z66xdpvq688ze2d8","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Let's start with a dense network for our baseline:","metadata":{"id":"DJ6QKG3hDMoa","cellId":"6yn15hpuolcmryork2oqs"}},{"cell_type":"code","source":"#!L\nmodel = nn.Sequential()\n\n# reshape from \"images\" to flat vectors\nmodel.add_module('flatten', Flatten())\n\n# dense \"head\"\nmodel.add_module('dense1', nn.Linear(3 * 64 * 64, 1064))\nmodel.add_module('dense2', nn.Linear(1064, 512))\nmodel.add_module('dropout0', nn.Dropout(0.05)) \nmodel.add_module('dense3', nn.Linear(512, 256))\nmodel.add_module('dropout1', nn.Dropout(0.05))\nmodel.add_module('dense4', nn.Linear(256, 64))\nmodel.add_module('dropout2', nn.Dropout(0.05))\nmodel.add_module('dense1_relu', nn.ReLU())\nmodel.add_module('dense2_logits', nn.Linear(64, 200)) # logits for 200 classes\n\n\nif torch.cuda.is_available():\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')\nmodel.to(device)\ndevice","metadata":{"id":"u_mbfRXMDMob","cellId":"f985tf2dvssqwmyc6w99d","trusted":true},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Flatten' is not defined","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)","\u001B[0;32m<ipython-input-1-21e7e7bb5fb9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# reshape from \"images\" to flat vectors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'flatten'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFlatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m# dense \"head\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mNameError\u001B[0m: name 'Flatten' is not defined"]}],"execution_count":8},{"cell_type":"markdown","source":"As in our basic tutorial, we train our model with negative log-likelihood aka crossentropy.","metadata":{"id":"DvugZZbeDMoe","cellId":"7dh3d8xmkeinv4kx0g079"}},{"cell_type":"code","source":"#!L\ndef compute_loss(X_batch, y_batch):\n    X_batch = torch.FloatTensor(X_batch).to(device=device)\n    y_batch = torch.LongTensor(y_batch).to(device=device)\n    logits = model.to(device)(X_batch)\n    return F.cross_entropy(logits, y_batch).mean()","metadata":{"id":"cGEhRWMYDMof","cellId":"3y7p7o6s7vecpf3kpktj8v"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training on minibatches\n* We got 100k images, that's way too many for a full-batch SGD. Let's train on minibatches instead\n* Below is a function that splits the training sample into minibatches","metadata":{"id":"kEhnKaujDMoi","cellId":"p4a6luymigbplsoliurw"}},{"cell_type":"code","source":"opt = torch.optim.SGD(model.parameters(), lr=0.01)\n\ntrain_loss = []\nval_accuracy = []","metadata":{"id":"GMA79nDODMoi","cellId":"lcktpsnetg10nod1roloyr"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nopt = torch.optim.SGD(model.parameters(), lr=0.01)\n\ntrain_loss = []\nval_accuracy = []\n\nnum_epochs = 50 # total amount of full passes over training data\n\nimport time\n\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    model.train(True) # enable dropout / batch_norm training behavior\n    for (X_batch, y_batch) in train_batch_gen:\n        # train on batch\n        loss = compute_loss(X_batch, y_batch)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        train_loss.append(loss.cpu().data.numpy())\n    \n    model.train(False) # disable dropout / use averages for batch_norm\n    for X_batch, y_batch in val_batch_gen:\n        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n        y_pred = logits.max(1)[1].data\n        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n\n    \n    # Then we print the results for this epoch:\n    print(\"Epoch {} of {} took {:.3f}s\".format(\n        epoch + 1, num_epochs, time.time() - start_time))\n    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n        np.mean(train_loss[-len(train_dataset) // batch_size :])))\n    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n        np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))","metadata":{"id":"sEy0LiHxDMol","cellId":"w8rht9ygh7uns89ypozln"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Don't wait for full 100 epochs. You can interrupt training after 5-20 epochs once validation accuracy stops going up.\n```\n```\n\n### Final test","metadata":{"id":"77q6ffk0DMon","cellId":"42doe8nw4umunw6t9k3m"}},{"cell_type":"code","source":"model.train(False) # disable dropout / use averages for batch_norm\ntest_batch_acc = []\nfor X_batch, y_batch in val_batch_gen:\n    logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n    y_pred = logits.max(1)[1].data\n    test_batch_acc.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n\n\ntest_accuracy = np.mean(test_batch_acc)\n    \nprint(\"Final results:\")\nprint(\"  test accuracy:\\t\\t{:.2f} %\".format(\n    test_accuracy * 100))\n\nif test_accuracy * 100 > 70:\n    print(\"U'r freakin' amazin'!\")\nelif test_accuracy * 100 > 50:\n    print(\"Achievement unlocked: 110lvl Warlock!\")\nelif test_accuracy * 100 > 40:\n    print(\"Achievement unlocked: 80lvl Warlock!\")\nelif test_accuracy * 100 > 30:\n    print(\"Achievement unlocked: 70lvl Warlock!\")\nelif test_accuracy * 100 > 20:\n    print(\"Achievement unlocked: 60lvl Warlock!\")\nelse:\n    print(\"We need more magic! Follow instructons below\")","metadata":{"id":"KmK71tpNDMoo","cellId":"3bwb0sgr9emg6lbxu9q0uw"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task I: small convolution net\n### First step\n\nLet's create a mini-convolutional network with roughly such architecture:\n* Input layer\n* 3x3 convolution with 128 filters and _ReLU_ activation\n* 2x2 pooling (or set previous convolution stride to 3)\n* Flatten\n* Dense layer with 1024 neurons and _ReLU_ activation\n* 30% dropout\n* Output dense layer.\n\n\n__Convolutional layers__ in torch are just like all other layers, but with a specific set of parameters:\n\n__`...`__\n\n__`model.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3)) # convolution`__\n\n__`model.add_module('pool1', nn.MaxPool2d(2)) # max pooling 2x2`__\n\n__`...`__\n\n\nOnce you're done (and compute_loss no longer raises errors), train it with __Adam__ optimizer with default params (feel free to modify the code above).\n\nIf everything is right, you should get at least __16%__ validation accuracy.\n\n__HACK_OF_THE_DAY__ :the number of channels must be in the order of the number of class_labels","metadata":{"id":"Cy06JedIDMos","cellId":"0z0tvct7nkypcm4qv6c00m"}},{"cell_type":"markdown","source":"### Before we start:\n**Stride, Padding and Kernel_size**","metadata":{"id":"jB5zePLiJBcw","cellId":"e966aarcrr6yn8ju7we72"}},{"cell_type":"code","source":"from IPython.display import Image\nImage(url='https://deeplearning.net/software/theano/_images/numerical_padding_strides.gif')  ","metadata":{"id":"RJLbJonGIUrQ","cellId":"ecg65c1ujoqbi7m9pp9rik"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = nn.Sequential()\n\nmodel.add_module\n#decribe convnet here\nmodel.add_module('flatten', Flatten())\nmodel.add_module('dense1_logits', nn.Linear(10368, 200)) # logits for 200 classes","metadata":{"id":"lEZfV3fIDMos","cellId":"hjs8r7vldzsoyxy4s4qkcp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt = torch.optim.SGD(model.parameters(), lr=0.01)\n\ntrain_loss = []\nval_accuracy = []","metadata":{"id":"SuIuiCmUDMov","cellId":"e5xtixbhyyavdeohu2xlj"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchsummary import summary\n\nsummary(model.cuda(), (3, 64, 64))","metadata":{"id":"32STsNW0DMox","cellId":"xzol5eh69mbh0y78gj04a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## retrain it ##","metadata":{"id":"4k_VEEp-DMoz","cellId":"fnkxzg6svarh6ljph9u13s"}},{"cell_type":"code","source":"import time\nnum_epochs = 100 # total amount of full passes over training data\nbatch_size = 50  # number of samples processed in one SGD iteration\n\n\nfor epoch in range(num_epochs):\n    print (num_epochs)\n    # In each epoch, we do a full pass over the training data:\n    start_time = time.time()\n    model.train(True) # enable dropout / batch_norm training behavior\n    for (X_batch, y_batch) in train_batch_gen:\n        # train on batch\n        loss = compute_loss(X_batch, y_batch)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        train_loss.append(loss.data.cpu().numpy())\n    print (num_epochs)    \n    model.train(False) # disable dropout / use averages for batch_norm\n    for X_batch, y_batch in val_batch_gen:\n        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n        y_pred = logits.max(1)[1].data\n        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n\n    print (num_epochs)\n    # Then we print the results for this epoch:\n    print(\"Epoch {} of {} took {:.3f}s\".format(\n        epoch + 1, num_epochs, time.time() - start_time))\n    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n        np.mean(train_loss[-len(train_dataset) // batch_size :])))\n    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n        np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))","metadata":{"id":"NAG-g8QrDMo0","cellId":"wodsy8etj2it77rny5u44l"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n```\n\n__Hint:__ If you don't want to compute shapes by hand, just plug in any shape (e.g. 1 unit) and run compute_loss. You will see something like this:\n\n__`RuntimeError: size mismatch, m1: [5 x 1960], m2: [1 x 64] at /some/long/path/to/torch/operation`__\n\nSee the __1960__ there? That's your actual input shape.\n\n## Task 2: adding normalization\n\n* Add batch norm (with default params) between convolution and ReLU\n  * nn.BatchNorm*d (1d for dense, 2d for conv)\n  * usually better to put them after linear/conv but before nonlinearity\n* Re-train the network with the same optimizer, it should get at least 20% validation accuracy at peak.\n\nTo know more about **batch_norm** and **data covariate shift**\n\nhttps://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n\nhttps://www.youtube.com/watch?v=nUUqwaxLnWs","metadata":{"id":"92eADk6qDMo2","cellId":"k1rhr6a3i5ictyja2ja09w"}},{"cell_type":"code","source":"model = nn.Sequential()\n\n#decribe conv net with batchnorm here","metadata":{"id":"LLbMCrwUDMo2","cellId":"h0orx2qeai7f8u5pns35zc"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt = torch.optim.SGD(model.parameters(), lr=0.01)\n\ntrain_loss = []\nval_accuracy = []","metadata":{"id":"LN2d5KN7DMo5","cellId":"opc2ig8mr4ps9jdz20yc3f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nnum_epochs = 100 # total amount of full passes over training data\nbatch_size = 50  # number of samples processed in one SGD iteration\n\n\nfor epoch in range(num_epochs):\n    print (num_epochs)\n    # In each epoch, we do a full pass over the training data:\n    start_time = time.time()\n    model.train(True) # enable dropout / batch_norm training behavior\n    for (X_batch, y_batch) in train_batch_gen:\n        # train on batch\n        loss = compute_loss(X_batch, y_batch)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        train_loss.append(loss.data.cpu().numpy())\n    print (num_epochs)    \n    model.train(False) # disable dropout / use averages for batch_norm\n    for X_batch, y_batch in val_batch_gen:\n        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n        y_pred = logits.max(1)[1].data\n        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n\n    print (num_epochs)\n    # Then we print the results for this epoch:\n    print(\"Epoch {} of {} took {:.3f}s\".format(\n        epoch + 1, num_epochs, time.time() - start_time))\n    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n        np.mean(train_loss[-len(train_dataset) // batch_size :])))\n    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n        np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))","metadata":{"id":"tnlGj42iDMo7","cellId":"rl7hxf77qdkrjahcysgf4"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n```\n\n```\n\n```\n\n```\n\n```\n## Task 3: Data Augmentation\n\n** Augmenti - A spell used to produce water from a wand (Harry Potter Wiki) **\n\n<img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/HagridsHut_PM_B6C28_Hagrid_sHutFireHarryFang.jpg?raw=1\" style=\"width:80%\">\n\nThere's a powerful torch tool for image preprocessing useful to do data preprocessing and augmentation.\n\nHere's how it works: we define a pipeline that\n* makes random crops of data (augmentation)\n* randomly flips image horizontally (augmentation)\n* then normalizes it (preprocessing)","metadata":{"id":"72fXRudFDMo9","cellId":"vf7qaq1wj4ajdcqghtdri"}},{"cell_type":"markdown","source":"When testing, we don't need random crops, just normalize with same statistics.","metadata":{"id":"_AG9EMuWDMo9","cellId":"slw9ya2dgwd07z4wt277s9l"}},{"cell_type":"code","source":"import torchvision\nfrom torchvision import transforms\n\ntransform_augment = <YOUR CODE>\n # decribe transformation here","metadata":{"id":"b8uC2R2PDMo-","cellId":"1k95do5uw063u9mq1l5kqy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=transform_augment)","metadata":{"id":"DTD34cgpDMpA","cellId":"rezdny8kd6hsacggoyow"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset, val_dataset = torch.utils.data.random_split(dataset, [90000, 10000])","metadata":{"id":"oBb0Gq6xDMpD","cellId":"d05majgvkitrqayuj5o7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=1)","metadata":{"id":"4Wu4eHU_DMpF","cellId":"68xeb753nfzs85u77cgd"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=1)","metadata":{"id":"cCOQEfSUDMpI","cellId":"630wr0cfmemfcdfd2lxzfl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nnum_epochs = 100 # total amount of full passes over training data\nbatch_size = 50  # number of samples processed in one SGD iteration\n\n\nfor epoch in range(num_epochs):\n    print (num_epochs)\n    # In each epoch, we do a full pass over the training data:\n    start_time = time.time()\n    model.train(True) # enable dropout / batch_norm training behavior\n    for (X_batch, y_batch) in train_batch_gen:\n        # train on batch\n        loss = compute_loss(X_batch, y_batch)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        train_loss.append(loss.data.cpu().numpy())\n    print (num_epochs)    \n    model.train(False) # disable dropout / use averages for batch_norm\n    for X_batch, y_batch in val_batch_gen:\n        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n        y_pred = logits.max(1)[1].data\n        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n\n    print (num_epochs)\n    # Then we print the results for this epoch:\n    print(\"Epoch {} of {} took {:.3f}s\".format(\n        epoch + 1, num_epochs, time.time() - start_time))\n    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n        np.mean(train_loss[-len(train_dataset) // batch_size :])))\n    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n        np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))","metadata":{"id":"dISTMNfcDMpK","cellId":"wt6332af48qebjbjtx82zi"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We need for test data __only normalization__, not cropping and rotation","metadata":{"id":"vQG4txuJDMpN","cellId":"3pj0hzqdxiif9s9ngjmtej"}},{"cell_type":"code","source":"transform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(means, stds), #normalize by channel. all value along the channel have mean and deviation\n])\n\ntest_dataset = <YOUR CODE>\n","metadata":{"id":"LntBFXN6DMpO","cellId":"hhz8nzik5gzpv9kq4xq9b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Quest For A Better Network\n\nSee `practical_dl/homework02` for a full-scale assignment.","metadata":{"id":"k7pl4SLpDMpQ","cellId":"nlsjsod2nx7nffpvf18efs"}}]}