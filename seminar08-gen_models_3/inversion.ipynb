{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "backend = 'None' # 'YC', 'Colab'\n",
    "\n",
    "if backend == 'Colab':\n",
    "    !pip install lpips\n",
    "    !git clone https://github.com/yandexdataschool/Practical_DL.git -b spring21\n",
    "    !sudo apt install -y ninja-build\n",
    "    %cd /content/Practical_DL/seminar07-gen_models_2\n",
    "    !wget https://www.dropbox.com/s/2kpsomtla61gjrn/pretrained.tar\n",
    "    !tar -xvf pretrained.tar\n",
    "elif backend == 'YC':\n",
    "    # Yandex Cloud (temporary unavailable)\n",
    "    %wget https://www.dropbox.com/s/2kpsomtla61gjrn/pretrained.tar\n",
    "    %tar -xvf pretrained.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/2kpsomtla61gjrn/pretrained.tar\n",
    "!tar -xvf pretrained.tar\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/Practical_DL/seminar08-gen_models_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print (torch.cuda.device_count())\n",
    "print (torch.__version__)\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm.auto import tqdm, trange\n",
    "from PIL import Image\n",
    "\n",
    "from gans.gan_load import make_stylegan2\n",
    "\n",
    "\n",
    "def to_image(tensor, adaptive=False):\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor[0]\n",
    "    if adaptive:\n",
    "        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    else:\n",
    "        tensor = ((tensor + 1) / 2).clamp(0, 1)\n",
    "\n",
    "    return ToPILImage()((255 * tensor.cpu().detach()).to(torch.uint8))\n",
    "\n",
    "\n",
    "def to_image_grid(tensor, adaptive=False, **kwargs):\n",
    "    return to_image(make_grid(tensor, **kwargs), adaptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /content/Practical_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "G = make_stylegan2(resolution=1024,\n",
    "                   weights='pretrained/stylegan2-ffhq-config-f.pt', target_key='g').eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([4, 512]).cuda()\n",
    "    imgs = G(z)\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.axis('off')\n",
    "plt.imshow(to_image_grid(imgs, nrow=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive inversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "\n",
    "zoom = 1.\n",
    "\n",
    "\n",
    "def portrait_crop(img, h_percent, w_percent):\n",
    "    w, h = img.size\n",
    "    w_offset = int(0.5 * (1 - w_percent) * w)\n",
    "    return img.crop([w_offset, 0, w - w_offset, int(h_percent * h)])\n",
    "\n",
    "\n",
    "def load_image(img_url, zoom=1.0, w=1.0, h=1.0):\n",
    "    crop = lambda x: portrait_crop(x, w, h)\n",
    "\n",
    "    normalization = transforms.Compose([\n",
    "        crop,\n",
    "        transforms.Resize(int(zoom * 1024)),\n",
    "        transforms.Resize(int(zoom * 1024)),\n",
    "        transforms.CenterCrop(1024),\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 2 * x - 1,\n",
    "    ])\n",
    "\n",
    "    img_data = requests.get(img_url).content\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "    return normalization(img).unsqueeze(0).cuda()\n",
    "\n",
    "\n",
    "imgs = []\n",
    "imgs.append(load_image('https://fotorelax.ru/wp-content/uploads/2015/08/Daniel-Jacob-Radcliffe_6.jpg'))\n",
    "imgs.append(load_image('https://www.kinogallery.com/pimages/742/kinogallery.com-742-520627.jpg', h=0.8))\n",
    "\n",
    "img = imgs[0]\n",
    "\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.axis('off')\n",
    "plt.imshow(to_image(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "lpips_model = lpips.LPIPS()\n",
    "lpips_model.cuda().eval()\n",
    "lpips_dist = lambda x, y: lpips_model(\n",
    "    F.interpolate(x, 256, mode='bilinear'),\n",
    "    F.interpolate(y, 256, mode='bilinear'))\n",
    "\n",
    "# CelebA regressor features extractor\n",
    "# same as at Seminar 7\n",
    "face_fe = torchvision.models.resnet18()\n",
    "face_fe.fc = nn.Sequential(nn.ReLU(), nn.Linear(512, 512), nn.ReLU())\n",
    "\n",
    "state_dict = torch.load('pretrained/regressor.pth')['model_state_dict']\n",
    "state_dict = {name[len('backbone.'):]: val for name, val in state_dict.items() if name.startswith('backbone.')}\n",
    "\n",
    "face_fe.load_state_dict(state_dict)\n",
    "face_fe.cuda().eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(img, G, latent_init, n_steps=500, lr=0.025,\n",
    "           l2_loss_scale=0.1, lpips_loss_scale=1.0, id_loss_scale=1.0,\n",
    "           latent_map=lambda x: x, **g_kwargs):\n",
    "    latent = nn.Parameter(latent_init.cuda())\n",
    "    opt = torch.optim.Adam([latent,], lr=lr)\n",
    "\n",
    "    l2_losses = []\n",
    "    perceptual_losses = []\n",
    "    id_losses = []\n",
    "    losses = []\n",
    "    for i in trange(n_steps):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        reconstruction = G(latent_map(latent), **g_kwargs)\n",
    "        l2_loss, perceptual_loss, id_loss = [torch.zeros([])] * 3\n",
    "        if l2_loss_scale > 0.0:\n",
    "            l2_loss = F.mse_loss(img, reconstruction).mean()\n",
    "        if lpips_loss_scale > 0.0:\n",
    "            perceptual_loss = lpips_dist(img, reconstruction).mean()\n",
    "        if id_loss_scale > 0.0:\n",
    "            id_loss = F.mse_loss(face_fe(img), face_fe(reconstruction)).mean()\n",
    "\n",
    "        loss = l2_loss_scale * l2_loss + lpips_loss_scale * perceptual_loss + id_loss_scale * id_loss\n",
    "        loss.backward()\n",
    "\n",
    "        l2_losses.append(l2_loss.item())\n",
    "        perceptual_losses.append(perceptual_loss.item())\n",
    "        id_losses.append(id_loss.item())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        opt.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'{i}: loss: {np.mean(losses[-100:]): 0.2f}; '\n",
    "                  f'l2-loss: {np.mean(l2_losses[-100:]): 0.2f}; '\n",
    "                  f'lpips loss: {np.mean(perceptual_losses[-100:]): 0.2f}; '\n",
    "                  f'id-loss: {np.mean(id_losses[-100:]): 0.2f}')\n",
    "\n",
    "    return reconstruction, latent, losses\n",
    "\n",
    "\n",
    "def show_inversion_result(img, reconstruction, losses=None):\n",
    "    _, axs = plt.subplots(1, 3, dpi=250)\n",
    "    for ax in axs[:2]: ax.axis('off')\n",
    "\n",
    "    axs[0].imshow(to_image_grid(img))\n",
    "    axs[1].imshow(to_image_grid(reconstruction))\n",
    "    if losses is not None:\n",
    "        axs[2].set_aspect(1.0 / np.max(losses) * len(losses))\n",
    "        axs[2].set_title('Loss')\n",
    "        axs[2].plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec, z, losses = invert(img, G, torch.randn([1, G.dim_z]), n_steps=100)\n",
    "show_inversion_result(img, rec, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mean = G.style_gan2.mean_latent(64)\n",
    "rec, w, losses = invert(img, G, w_mean, w_space=True, n_steps=100)\n",
    "show_inversion_result(img, rec, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mean = G.style_gan2.mean_latent(64)\n",
    "rec, w_plus, losses = invert(img, G, w_mean.unsqueeze(1).repeat(1, 18, 1), n_steps=100,\n",
    "                             latent_map=lambda w_plus: [w_plus], w_space=True)\n",
    "show_inversion_result(img, rec, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Style2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cc: https://github.com/eladrich/pixel2style2pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/eladrich/pixel2style2pixel\n",
    "!touch pixel2style2pixel/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('pixel2style2pixel')\n",
    "from models.encoders.psp_encoders import GradualStyleEncoder\n",
    "from argparse import Namespace\n",
    "\n",
    "encoder_chkpt = torch.load('pretrained/psp_ffhq_encode.pt')\n",
    "encoder = GradualStyleEncoder(50, 'ir_se', opts=Namespace(**encoder_chkpt['opts']))\n",
    "encoder_state = {name[len('encoder.'):]: val for name, val in encoder_chkpt['state_dict'].items() \\\n",
    "                 if name.startswith('encoder')}\n",
    "\n",
    "encoder.load_state_dict(encoder_state)\n",
    "encoder.cuda().eval();\n",
    "latent_mean = encoder_chkpt['latent_avg'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w_inversion = encoder(F.interpolate(img, 256, mode='bilinear')) + latent_mean[None]\n",
    "    rec = G([w_inversion], w_space=True)\n",
    "show_inversion_result(img, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pix2style2pix with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec, w_plus, losses = invert(img, G, w_inversion, n_steps=100, lr=0.005,\n",
    "                             latent_map=lambda w_plus: [w_plus], w_space=True)\n",
    "show_inversion_result(img, rec, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w_target = G.style_gan2.style(torch.randn([1, G.dim_z], device='cuda')).unsqueeze(1).repeat(1, 18, 1)\n",
    "    w_source = G.style_gan2.style(torch.randn([1, G.dim_z], device='cuda'))\n",
    "    target = G(w_target, w_space=True)\n",
    "    source = G(w_source, w_space=True)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(to_image_grid(torch.cat([target, source])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_rec = []\n",
    "for i in range(18):\n",
    "    w_styled = w_target.clone()\n",
    "    w_styled[:, :i] = w_source\n",
    "    styled_rec.append(G([w_styled], w_space=True).cpu().detach())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=250)\n",
    "plt.axis('off')\n",
    "plt.imshow(to_image_grid(torch.cat(styled_rec)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
